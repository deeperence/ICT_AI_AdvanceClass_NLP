{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This is a sample'),\n",
    "    ('Document2', 'This is another sample')\n",
    "]\n",
    "\n",
    "# in-memory (Hash Key 값)\n",
    "# 전체 색인어 목록(Dictionary)\n",
    "# {단어1:포스팅위치, 단어2:포스팅위치, ...}\n",
    "globalLexicon = dict()\n",
    "\n",
    "# 전체 문서 목록(Dictionary)\n",
    "# [0:문서1, 1:문서2, ...]\n",
    "globalDocument = list()\n",
    "\n",
    "# disk\n",
    "# 사전에 있는 색인어 중, 어느 문서에서, 몇 번 나타났는지\n",
    "# [(단어 idx, 문서 idx, 빈도, 다음주소), ...]\n",
    "# [0:Tuple(lexiconIdx, documentIdx, freq, 다음포스팅위치-fptr)]\n",
    "# 메모리 X, File OK\n",
    "globalPosting = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    localPosting = dict()\n",
    "    \n",
    "    # Local 작업\n",
    "    for term in docContent.lower().split():\n",
    "        # Local에 대해서, 없으면 추가\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "     \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for indexTerm, termFreq in localPosting.items():\n",
    "        if indexTerm not in globalLexicon.keys(): \n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, -1)\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = globalLexicon[indexTerm]\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, beforeIdx)\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx\n",
    "            \n",
    "#     print(localPosting)\n",
    "# print(globalDocument)\n",
    "\n",
    "#         if term not in globalLexicon.keys():\n",
    "#             lexiconIdx = len(globalLexicon) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 4, 'is': 5, 'a': 2, 'sample': 7, 'another': 6},\n ['Document1', 'Document2'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalLexicon, globalDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1, -1),\n (1, 0, 1, -1),\n (2, 0, 1, -1),\n (3, 0, 1, -1),\n (0, 1, 1, 0),\n (1, 1, 1, 1),\n (4, 1, 1, -1),\n (3, 1, 1, 3)]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n  DocName:Document2 - TermFreq:1 - Next:0\n  DocName:Document1 - TermFreq:1 - Next:-1\n\nis\n  DocName:Document2 - TermFreq:1 - Next:1\n  DocName:Document1 - TermFreq:1 - Next:-1\n\na\n  DocName:Document1 - TermFreq:1 - Next:-1\n\nsample\n  DocName:Document2 - TermFreq:1 - Next:3\n  DocName:Document1 - TermFreq:1 - Next:-1\n\nanother\n  DocName:Document2 - TermFreq:1 - Next:-1\n\n"
     ]
    }
   ],
   "source": [
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    # indexTerm:단어: postingIdx:위치, ...\n",
    "    print(indexTerm)\n",
    "    \n",
    "    while True: # Posting Next:-1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        print('  DocName:{0} - TermFreq:{1} - Next:{2}'.format(globalDocument[postingData[1]], postingData[2], postingData[3]))\n",
    "        postingIdx = postingData[3]\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1, 3)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalLexicon['sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 1, -1)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalPosting[globalLexicon['sample']][3]]   # 다음 주소가 \"-1\" 일때 까지 반복해서 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This is a a a a a a a a a a sample'),\n",
    "    ('Document2', 'This is a sample'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def binaryTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def rawTF(freq):\n",
    "    return freq\n",
    "\n",
    "def basicTF(freq, totalFreq):\n",
    "    return freq/totalFreq\n",
    "\n",
    "def logTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1+log10(freq)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def doubleNormalTF(K, freq, maxFreq): \n",
    "    return K + ((1-K) * (freq/maxFreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\nthis\n1. Binary:1\n2. Raw:1\n3. Basic:0.07692307692307693\n4. Log:1.0\n5. DoubleNormalization:0.1\n6. DoubleNormalization:0.55\n\nis\n1. Binary:1\n2. Raw:1\n3. Basic:0.07692307692307693\n4. Log:1.0\n5. DoubleNormalization:0.1\n6. DoubleNormalization:0.55\n\na\n1. Binary:1\n2. Raw:10\n3. Basic:0.7692307692307693\n4. Log:2.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\nsample\n1. Binary:1\n2. Raw:1\n3. Basic:0.07692307692307693\n4. Log:1.0\n5. DoubleNormalization:0.1\n6. DoubleNormalization:0.55\n\n{'this': 1, 'is': 1, 'a': 10, 'sample': 1}\n\n-----------------------------------\nthis\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\nis\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\na\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\nsample\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\n{'this': 1, 'is': 1, 'a': 1, 'sample': 1}\n\n"
     ]
    }
   ],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    localPosting = dict()\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "    \n",
    "    # localPosting => {단어:빈도, 단어:빈도, ...}\n",
    "    \n",
    "    maxFreq = max(localPosting.values())\n",
    "    totalCount = sum(localPosting.values())\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    for term, freq in localPosting.items():\n",
    "        print(term)\n",
    "        print('1. Binary:{0}'.format(binaryTF(freq)))\n",
    "        print('2. Raw:{0}'.format(rawTF(freq)))\n",
    "        print('3. Basic:{0}'.format(basicTF(freq, totalCount)))\n",
    "        print('4. Log:{0}'.format(logTF(freq)))\n",
    "        print('5. DoubleNormalization:{0}'.format(doubleNormalTF(0, freq, maxFreq)))\n",
    "        print('6. DoubleNormalization:{0}'.format(doubleNormalTF(0.5, freq, maxFreq)))\n",
    "        print()\n",
    "    \n",
    "    print(localPosting)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This a a a a a a a a a a  sample'),\n",
    "    ('Document2', 'This is a sample'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unaryIDF():\n",
    "    return 1\n",
    "\n",
    "def basicIDF(N, df):\n",
    "    return log10(N/df)\n",
    "\n",
    "def smoothigIDF(N, df):\n",
    "    return log10((N+1)/df)\n",
    "\n",
    "def probabilityIDF(N, df):\n",
    "    return log10((N-df+1)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\nis\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\na\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\nsample\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\nanother\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\nnot\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\n"
     ]
    }
   ],
   "source": [
    "N = len(collection)\n",
    "\n",
    "for term, ptr in globalLexicon.items():\n",
    "    # term:단어, ptr:위치, ...\n",
    "    df = 0\n",
    "    \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "    \n",
    "    print(term)\n",
    "    print('1. UnaryIDF: {0}'.format(unaryIDF()))\n",
    "    print('2. BasicIDF: {0}'.format(basicIDF(N,df)))\n",
    "    print('3. SmoothigIDF: {0}'.format(smoothigIDF(N,df)))\n",
    "    print('4. ProbabilityIDF: {0}'.format(probabilityIDF(N,df)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collection = [\n",
    "    ('Document1', 'This is a sample'),\n",
    "    ('Document2', 'This is another sample'),\n",
    "    ('Document3', 'This is not sample'),\n",
    "    ('Document4', 'a not sample'),\n",
    "    ('Document5', 'not'),\n",
    "    ('Document5', 'not sample')\n",
    "]\n",
    "\n",
    "globalLexicon = dict()\n",
    "globalDocument = list()\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    localPosting = dict()\n",
    "\n",
    "    for token in docContent.lower().split():\n",
    "        if token not in localPosting.keys():\n",
    "            localPosting[token] = 1\n",
    "        else:\n",
    "            localPosting[token] += 1\n",
    "            \n",
    "    maxFreq = max(localPosting.values())\n",
    "     \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for token, Freq in localPosting.items():\n",
    "        if token not in globalLexicon.keys(): \n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, freq, maxFreq), -1]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기\n",
    "            # tuple로 리턴받을경우 immutable(읽기전용)이라 list로 바꿔 주어야 함. \n",
    "            lexiconIdx = list(globalLexicon.keys()).index(token)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = globalLexicon[token]\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, freq, maxFreq), -1]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx\n",
    "            \n",
    "#     print(localPosting)\n",
    "# print(globalDocument)\n",
    "\n",
    "#         if term not in globalLexicon.keys():\n",
    "#             lexiconIdx = len(globalLexicon) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1.0, -1],\n [1, 0, 1.0, -1],\n [2, 0, 1.0, -1],\n [3, 0, 1.0, -1],\n [0, 1, 1.0, -1],\n [1, 1, 1.0, -1],\n [4, 1, 1.0, -1],\n [3, 1, 1.0, -1],\n [0, 2, 1.0, -1],\n [1, 2, 1.0, -1],\n [5, 2, 1.0, -1],\n [3, 2, 1.0, -1],\n [2, 3, 1.0, -1],\n [5, 3, 1.0, -1],\n [3, 3, 1.0, -1],\n [5, 4, 1.0, -1],\n [5, 5, 1.0, -1],\n [3, 5, 1.0, -1]]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 : this / IDF : 0.7781512503836436\n 문서 : Document3 / TF : 1.0 / TF-IDF : 0.7781512503836436\n단어 : is / IDF : 0.7781512503836436\n 문서 : Document3 / TF : 1.0 / TF-IDF : 0.7781512503836436\n단어 : a / IDF : 0.7781512503836436\n 문서 : Document4 / TF : 1.0 / TF-IDF : 0.7781512503836436\n단어 : sample / IDF : 0.7781512503836436\n 문서 : Document5 / TF : 1.0 / TF-IDF : 0.7781512503836436\n단어 : another / IDF : 0.7781512503836436\n 문서 : Document2 / TF : 1.0 / TF-IDF : 0.7781512503836436\n단어 : not / IDF : 0.7781512503836436\n 문서 : Document5 / TF : 1.0 / TF-IDF : 0.7781512503836436\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "N = len(globalDocument)\n",
    "idfList = dict()\n",
    "\n",
    "for term, ptr in globalLexicon.items():\n",
    "    # term:단어, ptr:위치, ...\n",
    "    df = 0\n",
    "    oldptr = ptr\n",
    "    \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "    ptr = oldptr\n",
    "    idf = basicIDF(N, df)\n",
    "    idfList[term] = idf # 나중에 단어가 나왔을때 idf를 불러올 수 있도록 하기 위함\n",
    "    print(\"단어 : {0} / IDF : {1}\".format(term, idf))\n",
    "    \n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[ptr]\n",
    "        tf = postingData[2]\n",
    "        postingData[2] = tf * idf\n",
    "        ptr = postingData[3]\n",
    "        print(\" 문서 : {0} / TF : {1} / TF-IDF : {2}\".format(\n",
    "            globalDocument[postingData[1]],\n",
    "            tf, postingData[2]))\n",
    "    idf = basicIDF(N, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1.0, -1],\n [1, 0, 1.0, -1],\n [2, 0, 1.0, -1],\n [3, 0, 1.0, -1],\n [0, 1, 1.0, -1],\n [1, 1, 1.0, -1],\n [4, 1, 0.7781512503836436, -1],\n [3, 1, 1.0, -1],\n [0, 2, 0.7781512503836436, -1],\n [1, 2, 0.7781512503836436, -1],\n [5, 2, 1.0, -1],\n [3, 2, 1.0, -1],\n [2, 3, 0.7781512503836436, -1],\n [5, 3, 1.0, -1],\n [3, 3, 1.0, -1],\n [5, 4, 1.0, -1],\n [5, 5, 0.7781512503836436, -1],\n [3, 5, 0.7781512503836436, -1]]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian(x, y):\n",
    "    return (x-y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"a\"\n",
    "queryPosting = dict()\n",
    "    \n",
    "for token in query.lower().split():\n",
    "    if token not in queryPosting.keys():\n",
    "        queryPosting[token] = 1\n",
    "    else:\n",
    "        queryPosting[token] += 1\n",
    "\n",
    "# localPosting => {단어:빈도, 단어:빈도, ...}\n",
    "\n",
    "maxFreq = max(queryPosting.values())\n",
    "\n",
    "for token, freq in queryPosting.items():\n",
    "    queryPosting[token] = doubleNormalTF(0.5, freq, maxFreq) * idfList[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 0.7781512503836436}"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = dict()\n",
    "\n",
    "for term, ptr in globalLexicon.items():\n",
    "    queryWeight = 0\n",
    "    \n",
    "    if term in queryPosting.keys(): # keys에 있는지 검사 후 query가 있을 경우에 한해 처리하기 위함\n",
    "        queryWeight = queryPosting[term]\n",
    "        \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "        # 키 에러 방지용\n",
    "        if postingData[1] in candidateList.keys():\n",
    "            candidateList[postingData[1]] += euclidian(queryWeight, postingData[2])\n",
    "        else:\n",
    "            candidateList[postingData[1]] = euclidian(queryWeight, postingData[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Document1', 'This is a sample'), ('Document2', 'This is another sample'), ('Document3', 'This is not sample'), ('Document4', 'a not sample'), ('Document5', 'not'), ('Document5', 'not sample')]\nCurrent query : ['a']\na\n순위 : 1 / 문서 : Document4 / 거리 : 0.0\na not sample\na\n순위 : 2 / 문서 : Document2 / 거리 : 0.6055193684736281\nThis is another sample\na\n순위 : 3 / 문서 : Document3 / 거리 : 1.2110387369472562\nThis is not sample\na\n순위 : 4 / 문서 : Document5 / 거리 : 1.2110387369472562\nnot sample\n"
     ]
    }
   ],
   "source": [
    "# 정렬 (reversed = False는 작은값부터 큰 값 순서)\n",
    "resultList = sorted(candidateList.items(), key=lambda x:x[1])\n",
    "\n",
    "print(collection) # 검색하고자 하는 문서 이름과 내용\n",
    "print(\"Current query :\",  [query])\n",
    "\n",
    "for i, (fileIdx, distance) in enumerate(resultList):\n",
    "    print(query)\n",
    "    print(\"순위 : {0} / 문서 : {1} / 거리 : {2}\".format(\n",
    "        (i+1), globalDocument[fileIdx], distance\n",
    "    ))\n",
    "    print(collection[fileIdx][1]) # 해당하는 문서도 출력\n",
    "    \n",
    "# query가 not일때와 a일때를 비교해 보면 유클리디안 거리의 문제점이 드러난다.(원노트 참고) 따라서 cosine simmilarity를 주로 사용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
