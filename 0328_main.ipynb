{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This is a sample'),\n",
    "    ('Document2', 'This is another sample')\n",
    "]\n",
    "\n",
    "# in-memory (Hash Key 값)\n",
    "# 전체 색인어 목록(Dictionary)\n",
    "# {단어1:포스팅위치, 단어2:포스팅위치, ...}\n",
    "queryPosting = dict()\n",
    "\n",
    "# 전체 문서 목록(Dictionary)\n",
    "# [0:문서1, 1:문서2, ...]\n",
    "globalDocument = list()\n",
    "\n",
    "# disk\n",
    "# 사전에 있는 색인어 중, 어느 문서에서, 몇 번 나타났는지\n",
    "# [(단어 idx, 문서 idx, 빈도, 다음주소), ...]\n",
    "# [0:Tuple(lexiconIdx, documentIdx, freq, 다음포스팅위치-fptr)]\n",
    "# 메모리 X, File OK\n",
    "globalPosting = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    localPosting = dict()\n",
    "    \n",
    "    # Local 작업\n",
    "    for term in docContent.lower().split():\n",
    "        # Local에 대해서, 없으면 추가\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "     \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for indexTerm, termFreq in localPosting.items():\n",
    "        if indexTerm not in queryPosting.keys(): \n",
    "            lexiconIdx = len(queryPosting)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, -1)\n",
    "            globalPosting.append(postingData)\n",
    "            queryPosting[indexTerm] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기\n",
    "            lexiconIdx = list(queryPosting.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = queryPosting[indexTerm]\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, beforeIdx)\n",
    "            globalPosting.append(postingData)\n",
    "            queryPosting[indexTerm] = postingIdx\n",
    "            \n",
    "#     print(localPosting)\n",
    "# print(globalDocument)\n",
    "\n",
    "#         if term not in globalLexicon.keys():\n",
    "#             lexiconIdx = len(globalLexicon) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'this': 4, 'is': 5, 'a': 2, 'sample': 7, 'another': 6},\n ['Document1', 'Document2'])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryPosting, globalDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0, 1, -1),\n (1, 0, 1, -1),\n (2, 0, 1, -1),\n (3, 0, 1, -1),\n (0, 1, 1, 0),\n (1, 1, 1, 1),\n (4, 1, 1, -1),\n (3, 1, 1, 3)]"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n  DocName:Document2 - TermFreq:1 - Next:0\n  DocName:Document1 - TermFreq:1 - Next:-1\n\nis\n  DocName:Document2 - TermFreq:1 - Next:1\n  DocName:Document1 - TermFreq:1 - Next:-1\n\na\n  DocName:Document1 - TermFreq:1 - Next:-1\n\nsample\n  DocName:Document2 - TermFreq:1 - Next:3\n  DocName:Document1 - TermFreq:1 - Next:-1\n\nanother\n  DocName:Document2 - TermFreq:1 - Next:-1\n\n"
     ]
    }
   ],
   "source": [
    "for indexTerm, postingIdx in queryPosting.items():\n",
    "    # indexTerm:단어: postingIdx:위치, ...\n",
    "    print(indexTerm)\n",
    "    \n",
    "    while True: # Posting Next:-1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        print('  DocName:{0} - TermFreq:{1} - Next:{2}'.format(globalDocument[postingData[1]], postingData[2], postingData[3]))\n",
    "        postingIdx = postingData[3]\n",
    "        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1, 1, 3)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[queryPosting['sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 0, 1, -1)"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalPosting[queryPosting['sample']][3]]   # 다음 주소가 \"-1\" 일때 까지 반복해서 찾음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This is a a a a a a a a a a sample'),\n",
    "    ('Document2', 'This is a sample'),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "def binaryTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def rawTF(freq):\n",
    "    return freq\n",
    "\n",
    "def basicTF(freq, totalFreq):\n",
    "    return freq/totalFreq\n",
    "\n",
    "def logTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1+log10(freq)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def doubleNormalTF(K, freq, maxFreq): \n",
    "    return K + ((1-K) * (freq/maxFreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\nthis\n1. Binary:1\n2. Raw:1\n3. Basic:0.07692307692307693\n4. Log:1.0\n5. DoubleNormalization:0.1\n6. DoubleNormalization:0.55\n\nis\n1. Binary:1\n2. Raw:1\n3. Basic:0.07692307692307693\n4. Log:1.0\n5. DoubleNormalization:0.1\n6. DoubleNormalization:0.55\n\na\n1. Binary:1\n2. Raw:10\n3. Basic:0.7692307692307693\n4. Log:2.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\nsample\n1. Binary:1\n2. Raw:1\n3. Basic:0.07692307692307693\n4. Log:1.0\n5. DoubleNormalization:0.1\n6. DoubleNormalization:0.55\n\n{'this': 1, 'is': 1, 'a': 10, 'sample': 1}\n\n-----------------------------------\nthis\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\nis\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\na\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\nsample\n1. Binary:1\n2. Raw:1\n3. Basic:0.25\n4. Log:1.0\n5. DoubleNormalization:1.0\n6. DoubleNormalization:1.0\n\n{'this': 1, 'is': 1, 'a': 1, 'sample': 1}\n\n"
     ]
    }
   ],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    localPosting = dict()\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "    \n",
    "    # localPosting => {단어:빈도, 단어:빈도, ...}\n",
    "    \n",
    "    maxFreq = max(localPosting.values())\n",
    "    totalCount = sum(localPosting.values())\n",
    "    \n",
    "    print('-----------------------------------')\n",
    "    \n",
    "    for term, freq in localPosting.items():\n",
    "        print(term)\n",
    "        print('1. Binary:{0}'.format(binaryTF(freq)))\n",
    "        print('2. Raw:{0}'.format(rawTF(freq)))\n",
    "        print('3. Basic:{0}'.format(basicTF(freq, totalCount)))\n",
    "        print('4. Log:{0}'.format(logTF(freq)))\n",
    "        print('5. DoubleNormalization:{0}'.format(doubleNormalTF(0, freq, maxFreq)))\n",
    "        print('6. DoubleNormalization:{0}'.format(doubleNormalTF(0.5, freq, maxFreq)))\n",
    "        print()\n",
    "    \n",
    "    print(localPosting)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection = [\n",
    "    ('Document1', 'This a a a a a a a a a a  sample'),\n",
    "    ('Document2', 'This is a sample'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unaryIDF():\n",
    "    return 1\n",
    "\n",
    "def basicIDF(N, df):\n",
    "    return log10(N/df)\n",
    "\n",
    "def smoothigIDF(N, df):\n",
    "    return log10((N+1)/df)\n",
    "\n",
    "def probabilityIDF(N, df):\n",
    "    return log10((N-df+1)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this\n1. UnaryIDF: 1\n2. BasicIDF: 0.0\n3. SmoothigIDF: 0.17609125905568124\n4. ProbabilityIDF: -0.3010299956639812\n\nis\n1. UnaryIDF: 1\n2. BasicIDF: 0.0\n3. SmoothigIDF: 0.17609125905568124\n4. ProbabilityIDF: -0.3010299956639812\n\na\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\nsample\n1. UnaryIDF: 1\n2. BasicIDF: 0.0\n3. SmoothigIDF: 0.17609125905568124\n4. ProbabilityIDF: -0.3010299956639812\n\nanother\n1. UnaryIDF: 1\n2. BasicIDF: 0.3010299956639812\n3. SmoothigIDF: 0.47712125471966244\n4. ProbabilityIDF: 0.3010299956639812\n\n"
     ]
    }
   ],
   "source": [
    "N = len(collection)\n",
    "\n",
    "for term, ptr in queryPosting.items():\n",
    "    # term:단어, ptr:위치, ...\n",
    "    df = 0\n",
    "    \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "    \n",
    "    print(term)\n",
    "    print('1. UnaryIDF: {0}'.format(unaryIDF()))\n",
    "    print('2. BasicIDF: {0}'.format(basicIDF(N,df)))\n",
    "    print('3. SmoothigIDF: {0}'.format(smoothigIDF(N,df)))\n",
    "    print('4. ProbabilityIDF: {0}'.format(probabilityIDF(N,df)))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "collection = [\n",
    "    ('Document1', 'This is a sample'),\n",
    "    ('Document2', 'This is another sample'),\n",
    "    ('Document3', 'This is not sample'),\n",
    "    ('Document4', 'a not sample'),\n",
    "    ('Document5', 'not'),\n",
    "    ('Document5', 'not sample')\n",
    "]\n",
    "\n",
    "globalLexicon = dict()\n",
    "globalDocument = list()\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    localPosting = dict()\n",
    "\n",
    "    for token in docContent.lower().split():\n",
    "        if token not in localPosting.keys():\n",
    "            localPosting[token] = 1\n",
    "        else:\n",
    "            localPosting[token] += 1\n",
    "            \n",
    "    maxFreq = max(localPosting.values())\n",
    "     \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for token, Freq in localPosting.items():\n",
    "        if token not in globalLexicon.keys(): \n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, freq, maxFreq), -1]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기\n",
    "            # tuple로 리턴받을경우 immutable(읽기전용)이라 list로 바꿔 주어야 함. \n",
    "            lexiconIdx = list(globalLexicon.keys()).index(token)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = globalLexicon[token]\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, freq, maxFreq), -1]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[token] = postingIdx\n",
    "            \n",
    "#     print(localPosting)\n",
    "# print(globalDocument)\n",
    "\n",
    "#         if term not in globalLexicon.keys():\n",
    "#             lexiconIdx = len(globalLexicon) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1.0, -1],\n [1, 0, 1.0, -1],\n [2, 0, 1.0, -1],\n [3, 0, 1.0, -1],\n [0, 1, 1.0, -1],\n [1, 1, 1.0, -1],\n [4, 1, 1.0, -1],\n [3, 1, 1.0, -1],\n [0, 2, 1.0, -1],\n [1, 2, 1.0, -1],\n [5, 2, 1.0, -1],\n [3, 2, 1.0, -1],\n [2, 3, 1.0, -1],\n [5, 3, 1.0, -1],\n [3, 3, 1.0, -1],\n [5, 4, 1.0, -1],\n [5, 5, 1.0, -1],\n [3, 5, 1.0, -1]]"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not float",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-279-bc1fdd4f7092>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mdf\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mpostingData\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglobalPosting\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mptr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mptr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpostingData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: list indices must be integers or slices, not float"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "N = len(globalDocument)\n",
    "idfList = dict()\n",
    "documentLength = dict()\n",
    "\n",
    "for term, ptr in queryPosting.items():\n",
    "    # term:단어, ptr:위치, ...\n",
    "    df = 0\n",
    "    oldptr = ptr\n",
    "    \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "    ptr = oldptr\n",
    "    idf = basicIDF(N, df)\n",
    "    idfList[term] = idf # 나중에 단어가 나왔을때 idf를 불러올 수 있도록 하기 위함\n",
    "    print(\"단어 : {0} / IDF : {1}\".format(term, idf))\n",
    "    \n",
    "    while True:\n",
    "        if ptr == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[ptr]\n",
    "        tf = postingData[2]\n",
    "        postingData[2] = tf * idf\n",
    "        ptr = postingData[3]\n",
    "        print(\" 문서 : {0} / TF : {1} / TF-IDF : {2}\".format(\n",
    "            globalDocument[postingData[1]],\n",
    "            tf, postingData[2]))\n",
    "        \n",
    "        # 색인하는 과정에서 사실상 검색까지 모두 한 셈이 되므로 색인 과정에만 시간이 들어 굉장히 빠른 연산속도를 보임. \n",
    "        if postingData[1] in documentLength.keys(): # 키값이 있다면 제곱해서 누적해주면 됨\n",
    "            documentLength[postingData[1]] += postingData[2] ** 2\n",
    "        else:\n",
    "            documentLength[postingData[1]] = postingData[2] ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documentLength"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 1.0, -1],\n [1, 0, 1.0, -1],\n [2, 0, 1.0, -1],\n [3, 0, 1.0, -1],\n [0, 1, 1.0, -1],\n [1, 1, 1.0, -1],\n [4, 1, 1.0, -1],\n [3, 1, 1.0, -1],\n [0, 2, 1.0, -1],\n [1, 2, 1.0, -1],\n [5, 2, 1.0, -1],\n [3, 2, 1.0, -1],\n [2, 3, 1.0, -1],\n [5, 3, 1.0, -1],\n [3, 3, 1.0, -1],\n [5, 4, 1.0, -1],\n [5, 5, 1.0, -1],\n [3, 5, 1.0, -1]]"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian(x, y):\n",
    "    return (x-y) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-283-771877afdc22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mqueryPosting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mqueryPosting\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdoubleNormalTF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxFreq\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0midfList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 'a'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "query = \"a\"\n",
    "queryPosting = dict()\n",
    "    \n",
    "for token in query.lower().split():\n",
    "    if token not in queryPosting.keys():\n",
    "        queryPosting[token] = 1\n",
    "    else:\n",
    "        queryPosting[token] += 1\n",
    "\n",
    "# localPosting => {단어:빈도, 단어:빈도, ...}\n",
    "\n",
    "maxFreq = max(queryPosting.values())\n",
    "\n",
    "for token, freq in queryPosting.items():\n",
    "    queryPosting[token] = doubleNormalTF(0.5, freq, maxFreq) * idfList[token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1}"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queryPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = dict()\n",
    "\n",
    "for term, ptr in queryPosting.items():\n",
    "    queryWeight = 0\n",
    "    \n",
    "    if term in queryPosting.keys(): # keys에 있는지 검사 후 query가 있을 경우에 한해 처리하기 위함\n",
    "        queryWeight = queryPosting[term]\n",
    "        \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "        # 키 에러 방지용\n",
    "        if postingData[1] in candidateList.keys():\n",
    "            candidateList[postingData[1]] += euclidian(queryWeight, postingData[2])\n",
    "        else:\n",
    "            candidateList[postingData[1]] = euclidian(queryWeight, postingData[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Document1', 'This is a sample'), ('Document2', 'This is another sample'), ('Document3', 'This is not sample'), ('Document4', 'a not sample'), ('Document5', 'not'), ('Document5', 'not sample')]\nCurrent query : ['a']\na\n순위 : 1 / 문서 : Document1 / 거리 : 0.0\nThis is a sample\n"
     ]
    }
   ],
   "source": [
    "# 정렬 (reversed = False는 작은값부터 큰 값 순서)\n",
    "resultList = sorted(candidateList.items(), key=lambda x:x[1])\n",
    "\n",
    "print(collection) # 검색하고자 하는 문서 이름과 내용\n",
    "print(\"Current query :\",  [query])\n",
    "\n",
    "for i, (fileIdx, distance) in enumerate(resultList):\n",
    "    print(query)\n",
    "    print(\"순위 : {0} / 문서 : {1} / 거리 : {2}\".format(\n",
    "        (i+1), globalDocument[fileIdx], distance\n",
    "    ))\n",
    "    print(collection[fileIdx][1]) # 해당하는 문서도 출력\n",
    "    \n",
    "# query가 not일때와 a일때를 비교해 보면 유클리디안 거리의 문제점이 드러난다.(원노트 참고) 따라서 cosine simmilarity를 주로 사용. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def innerProduct(x, y):\n",
    "    return x * y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = dict()\n",
    "\n",
    "# 전체 lexicon을 고려할 필요 없이 query에 있는것만 고려하면 됨. \n",
    "for term, weight in queryPosting.items():\n",
    "    ptr = globalLexicon[term]\n",
    "        \n",
    "    while True:    # ptr Next: -1\n",
    "        if ptr == -1:\n",
    "            break\n",
    "        postingData = globalPosting[ptr]\n",
    "        ptr = postingData[3]\n",
    "        \n",
    "        # 키 에러 방지용\n",
    "        if postingData[1] in candidateList.keys():\n",
    "            candidateList[postingData[1]] += innerProduct(weight, postingData[2])\n",
    "        else:\n",
    "            candidateList[postingData[1]] = innerProduct(weight, postingData[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "3",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-295-49c8d9144fc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 길이 1로 normalize하는과정\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfileIdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msumProduct\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcandidateList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mcandidateList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfileIdx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msumProduct\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdocumentLength\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfileIdx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m: 3"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# 길이 1로 normalize하는과정\n",
    "for fileIdx, sumProduct in candidateList.items():\n",
    "    candidateList[fileIdx] = sumProduct / documentLength[fileIdx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query :  a\na\n순위 : 1 / 문서 : Document1 / 거리 : 0.0\nThis is a sample\n"
     ]
    }
   ],
   "source": [
    "result = sorted(candidateList.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "print(\"Query : \", query)\n",
    "\n",
    "for i, (fileIdx, distance) in enumerate(resultList):\n",
    "    print(query)\n",
    "    print(\"순위 : {0} / 문서 : {1} / 거리 : {2}\".format(\n",
    "        (i+1), globalDocument[fileIdx], distance\n",
    "    ))\n",
    "    print(collection[fileIdx][1]) # 해당하는 문서도 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "morphs() missing 1 required positional argument: 'phrase'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-334-6fc9d01f5f98>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkonlpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mKkma\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKkma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmorphs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgetUniqueTermsBySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: morphs() missing 1 required positional argument: 'phrase'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from konlpy.corpus import kobill\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "ma = Kkma().morphs()\n",
    "\n",
    "def getUniqueTermsBySet():\n",
    "    lexicon = list()\n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName).read()\n",
    "        for token in documentLength.split():\n",
    "            lexicon.extend(ma(token))\n",
    "    return list(set(lexicon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getUniqueTermsBySet' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-335-4c5473ddfbd6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlexicon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetUniqueTermsBySet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'getUniqueTermsBySet' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "lexicon = getUniqueTermsBySet() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lexicon' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-336-0800d0f45c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'lexicon' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def docRepresentationByDefaultDict():\n",
    "    docList = defaultdict(lambda: defaultdict(int))\n",
    "    \n",
    "    for fileName in kobill.fileids():\n",
    "        document = kobill.open(fileName.read())\n",
    "        \n",
    "        for token in document.split():\n",
    "            for morpheme in ma(token):  \n",
    "                docList[fileName][morpheme] =1\n",
    "    return docList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'read'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-338-65fb1686ba87>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mDTM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocRepresentationByDefaultDict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-337-58f1b0ab0c27>\u001b[0m in \u001b[0;36mdocRepresentationByDefaultDict\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfileName\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkobill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mdocument\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkobill\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "DTM = docRepresentationByDefaultDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invertedDocument(DTM):\n",
    "    TDM = defaultdict(lambda: defaultdict(float))\n",
    "    \n",
    "    for docName, docVector in DTM.items():  \n",
    "        maxFreq = max(docVector.values())\n",
    "        for term, freq in docVector.items():\n",
    "            TDM[term][docName] = doubleNormalTF(0, freq, maxFreq)\n",
    "            \n",
    "    return TDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DTM' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-340-0a0ab4989841>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTDM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvertedDocument\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDTM\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# 0322_main.ipynb와 달리 빈도와 가중치를 동시에 갖게 된다.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'DTM' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "TDM = invertedDocument(DTM) # 0322_main.ipynb와 달리 빈도와 가중치를 동시에 갖게 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DTM' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-341-35582a78c6c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDTM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0minvertedWeight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTDM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# TDM을 받아 term에 대한 weight가 들어있는 값으로 리턴\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mTWM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mDVL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'DTM' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "N = len(DTM) \n",
    "\n",
    "def invertedWeight(TDM): # TDM을 받아 term에 대한 weight가 들어있는 값으로 리턴\n",
    "    TWM = defaultdict(lambda: defaultdict(float))\n",
    "    DVL = defaultdict(float)\n",
    "    \n",
    "    for term, tfList in TDM.items():\n",
    "        df = len(tfList)\n",
    "        idf = basicIDF(N, df)\n",
    "        \n",
    "        for docName, tf in tfList.items():\n",
    "            # 각 문서에서 각 단어가 차지하는 단어\n",
    "            TWM[term][docName] = tf*idf \n",
    "            DVL[docName] += TWM[term][docName] ** 2\n",
    "        \n",
    "    return TWM, DVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'invertedWeight' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-342-ae0920e88e09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mTWM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDVL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minvertedWeight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTDM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'invertedWeight' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "TWM, DVL = invertedWeight(TDM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 12)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<tokenize>\"\u001b[1;36m, line \u001b[1;32m12\u001b[0m\n\u001b[1;33m    if df> 0:\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "query = \"국방의 의무와 보편적 교육에 대한 법안\"\n",
    "queryRepresentation = defaultdict(float)\n",
    "\n",
    "for token in query.split():\n",
    "    for morpheme in ma(token):\n",
    "        queryRepresentation[morpheme] += 1 # 빈도도 쉽게 구할 수 있다. \n",
    "\n",
    "maxFreq = max(queryRepresentation.values())\n",
    "\n",
    "for term, freq in queryRepresentation.items():\n",
    "        df = len(TWM[term])\n",
    "    if df> 0:\n",
    "        idf = basicIDF(N, df)\n",
    "        queryRepresentation[term] = doubleNormalTF(0.5, freq, maxFreq)\n",
    "    else:\n",
    "        queryRepresentation[term] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidateList = defaultdict(float)\n",
    "\n",
    "for term, weight in queryRepresentation.items():\n",
    "    for docName, docWeight in TWM[term].items():\n",
    "        candidateList[docName] += weight * docWeight\n",
    "        \n",
    "for docName, simmilarity in candidateList.items():\n",
    "    candidateList[docName] = simmilarity / DVL[docName]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query :  국방의 의무와 보편적 교육에 대한 법안\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize # splitlines 대신에 사용할 목적\n",
    "resultList = sorted(candidateList.items(), key=lambda x:x[1], reverse=True)\n",
    "\n",
    "print(\"Query : \", query)\n",
    "\n",
    "for i, (fileIdx, distance) in enumerate(resultList):\n",
    "    print(query)\n",
    "    print(\"순위 : {0} / 문서 : {1} / 거리 : {2}\".format(\n",
    "        (i+1), globalDocument[fileIdx], simmilarity\n",
    "    ))\n",
    "    \n",
    "    content = kobill.open(docName).read()\n",
    "    # print(content.splitlines()[:5])\n",
    "    print(sent_tokenize(content)[:4])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
