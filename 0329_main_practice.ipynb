{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 색인 & 가중치 => 학습\n",
    "1. 수집한 문서 목록 가져오기\n",
    "2. 각 문서에서 색인어 목록 추출 전처리 -> 어절 -> 형태소+명사+바이그램\n",
    "3. 가중치 계산(TF-IDF) -> TDM -> TWM(with DocumentVectorLength)\n",
    "\n",
    "* 질의 -> 색인과정의 2 to 3\n",
    "4. 질의에서 색인어 추출\n",
    "5. 가중치 계산\n",
    "6. 유사도 계산(코싸인)\n",
    "7. 유사도 순 정렬(거리-오름차순, 각도-내림차순)\n",
    "8. 검색 결과 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 모든 문서들에 대해 토큰화 수행하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 코드에서는 지금까지 배운 내용들을 활용해 텍스트 문서에서 쿼리 단어를 검색한 후 해당 쿼리가 위치한 문서 경로를 출력해 봅니다. \n",
    "\n",
    "def getFileList(base = \"./\", ext = \"txt\"): # 아무것도 안했다면 base는 현재 경로\n",
    "    fileList = list()\n",
    "    for file in listdir(base):\n",
    "        if file.split(\".\")[-1] == ext: # .을 기준으로 split한 것이 txt인지 검사\n",
    "            fileList.append(\"{0}/{1}\".format(base, file))\n",
    "            \n",
    "    return fileList\n",
    "\n",
    "# txt 컨텐츠를 편하게 읽어오기 위한 함수\n",
    "def getContent(file): \n",
    "    with open(file, encoding=\"UTF-8\") as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramEojeol(sentence, n=2): # sentence를 받아 어절 단위로 분리해주는 함수\n",
    "    '''\n",
    "    입력:     단어1,   단어2,   단어3,  단어4 : 4\n",
    "    출력(2) : 단어12,  단어23,  단어34 :        3 - n + 1\n",
    "    출력(3) : 단어123, 단어234         :        2 - n + 1\n",
    "    '''\n",
    "    tokens = sentence.split()\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram.append(' '.join(tokens[i:i + n]))    \n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramUmjeol(term, n = 2): # 음절 단위로 구분하는 함수. sentence를 받아 2개(n=2)씩 쪼갠다.\n",
    "\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(term) - n + 1):\n",
    "        # ngram.append(tokens_ngram[i:i+n]) # 방법1\n",
    "        # ngram.append(tuple(tokens_ngram[i:i+n])) # 방법2 (튜플로 반환 시 키값을 쓸 수 있음)\n",
    "        ngram.append(''.join(term[i:i + n])) # 방법3\n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def getPatternList():\n",
    "    patternList = dict()\n",
    "    pattern = re.compile(r\"[%s]{1,}\" % re.escape(punctuation)) # punctuation 안의 특수문자가 두번이상 반복되는 모든 문자에 대해 패턴 정의\n",
    "    patternList[\"Punctuation\"] = pattern\n",
    "    pattern2 = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{3,}@[A-Za-z0-9\\-\\_]{3,}(.[A-Za-z]{2,})+)\") #이메일주소제거패턴\n",
    "    patternList[\"Email\"] = pattern2\n",
    "    pattern3 = re.compile(r\"([A-Za-z0-9\\-\\_]{1,}(.[A-Za-z]{2,})+)\") # 신문사도메인패턴\n",
    "    patternList[\"Domain\"] = pattern3\n",
    "    pattern4 = re.compile(r\"\\s{2,}\") # 공백제거\n",
    "    patternList[\"Whitespace\"] = pattern4\n",
    "    pattern5 = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣0-9]+)\") # 한글이 아닌 영어 기호 제거\n",
    "    patternList[\"Korean\"] = pattern5\n",
    "    pattern6 = re.compile(r'[A-Za-z-_]{8,}') # 영어 (대소문자) +\\-\\_ 제거 (8글자 이상)\n",
    "    patternList[\"ElimLongEng\"] = pattern6\n",
    "    pattern7 = re.compile(r\"를 2019 및 금지|네이버 채널에서|저작권자 주 블로터앤미디어 저작권자를 명기하고 내용을 변경하지 않으며 비상업적으로 이용하는 조건아래 가능합니다|\"\n",
    "                          r\"디지털 마케팅의 미래 아이포럼|무단전재|오류를 우회하기 위한 함수 추가|재배포|기자|아이뉴스24|구독해주세요|\"\n",
    "                          r\"고수들의 재테크 비법|이건희칼럼|성공을 꿈꾸는 사람들의 경제 뉴스|머니 및 금지|및 금지|\"\n",
    "                          r\"네이버 모바일에서|네이버에서 헤럴드경제|채널 구독하기|헤럴드 리얼라이프 헤럴드경제 사이트 바로가기 헤럴드경제 무단 전재 및 금지|\"\n",
    "                          r\"전자신문|바로가기|전자신문인터넷|인터넷 및 금지|중앙일보|친구추가|구독 1위|\"\n",
    "                          r\"한국경제|재배포|기사제보|보도자료|한경닷컴|네이버에서|자세히보기|자세히 보기|모바일한경|구독신청|\"\n",
    "                          r\"머니투데이|돈이 보이는 리얼타임 뉴스|연예뉴스|파이낸셜뉴스|무단 전재 금지\") # 높은 가중치를 갖는 단어를 하드코딩 데이터로 제거\n",
    "    patternList[\"ElimRecWord\"] = pattern7\n",
    "    \n",
    "    return patternList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "FileList = getFileList(\"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\")\n",
    "NewsContent = list() \n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    NewsContent.append(getContent(FileList[i])) # len(NewsContent) = 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 오히려 단체측 무선이어폰 연구는 존재치도 않아 국내 참여 연구자도 어리둥절 이엠에프사이언티스에서 일부 언론 보도를 부정하며 직접 보내온 이메일 내용이다 이엠에프사이언티스트 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 무선 이어폰의 건강 유해성에 대한 보도를 부정했다 고재원 1212 18일 오전 전세계 과학자들이 애플 에어팟과 같은 무선 이어폰이 암 발생 위험을 키울 수 있다는 호소문을 국제연합 과 세계보건기구 에 제출했다는 일부 국내외 언론의 보도가 나오면서 불안감이 확산되고 있다 하지만 실제 이 단체와 호소문에 이름을 올린 과학자들에게 확인한 결과 호소문은 4년전 제출됐던 것이며 또 특정 제품이나 제조사를 언급하지 않은 것으로 확인됐다 18일 데일리메일과 등 국내외 일부 언론에 따르면 전 세계 과학자 247명이 무선 이어폰의 비이온화 전자기장 이 암을 유발할 위험 우려가 있다며 과 에 호소문을 제출했다고 전했다 이들 매체들은 호소문에 애플 에어팟이 에 관한 법적 기준치를 준수하고 있지만 장시간 노출될 경우 건강에 좋지 않을 수 있다 는 내용이 포함됐다고 보도했다 이들 매체들은 이 호소문에는 전 세계 42개국 과학자 247명이 서명을 했다고 전했다 여기에는 한국의 연세대 한양대 가톨릭대 단국대 중앙대 경북대 한림대 소속 과학자 15명의 이름도 포함됐다 하지만 취재 결과 호소문 작성을 주도한 비영리단체 이엠에프사이언티스트 는 애플 에어팟과 같은 무선 이어폰에 대한 유해성을 주장하지 않은 것으로 확인됐다 동아사이언스가 이엠에프사이언티스트에 직접 이메일로 확인한 결과 일부 언론 보도가 호소문에 대한 부정확한 내용을 담고 있다 며 무선 블루투스의 자기장에 머리가 장시간 노출될 시의 안정성에 대한 연구는 존재하지 않는다 고 밝혔다 이엠에프사이언티스트에 따르면 이 단체는 지난 2015년 5월 실제로 전세계 과학자 190명의 서명을 받아 과 유엔환경계획 에 국제 과학자 호소문 을 제출하기는 했다 당시 호소문에는 전세계 저명한 학술지에 게재된 2000개가 넘는 연구들을 근거로 비전리 전자기장 노출로부터 보호와 방지가 필요하다는 내용을 담았다 이 단체는 지속적으로 해당 내용에 동의하는 과학자들의 서명을 받아 현재는 42개국 247명으로 늘어났다 실제 한국인 과학자들 15명도 이 호소문에 서명한 것으로 확인됐다 이엠에프사이언티스 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 이메일 인터뷰에서 호소문에 서명한 과학자들은 저명한 학술지에 전자기장과 생물학 건강과 관련된 연구를 발표한 분들 이라며 하지만 호소문은 이번에 새로 제출된 내용이 아닌 2015년 5월 제출됐으며 특정 제품이나 제조사를 언급하고 있지 않다 고 말했다 이 호소문에 서명했다고 알려진 송기원 연세대 생화학과 교수는 명단이 올라간 것도 잘 몰랐다 며 최근 몇 년 동안 해당 주제에 관여하지 않았고 당시도 다른 교수가 서명하라 해 서명했다 고 말했다 또 다른 서명자인 한 지방국립대 교수는 대략 2년전 서명했던 기억이 난다 고 말했다 전자기장 자체 유해성 여부는 아직까지 논란이 있다 는 지난 30년간 2만5000개가 넘는 관련 연구들이 존재하지만 낮은 전자기장의 건강 유해성을 확인하지 못했다고 결론내렸다 이덕환 서강대 화학과 교수는 통신용 마이크로파가 인체에 영향을 미치는지 여부는 이미 여러 연구를 통해 문제가 없다고 확실히 검증됐다 며 이미 우리 주변에 수많은 통신용 마이크로파가 지나다니고 있는데 전화나 무선이어폰을 가까이 가져간다고 새삼 더 문제가 된다는 발상도 황당하다 고 밝혔다 고재원 1212 \n"
     ]
    }
   ],
   "source": [
    "patternList = getPatternList()\n",
    "for _ in [\"Korean\", \"Whitespace\", \"Punctuation\", \"ElimLongEng\", \"Email\", \"Domain\", \"ElimRecWord\"]: \n",
    "    for i in range(len(FileList)):\n",
    "        NewsContent[i] = patternList[_].sub(\" \", NewsContent[i]) \n",
    "print(NewsContent[0]) # 불용어 처리가 모두 끝난 뉴스기사 목록\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from konlpy.tag import Kkma\n",
    "ma = Kkma()\n",
    "\n",
    "# 단일 뉴스 기사를 토큰화해주는 함수\n",
    "def Tokenizer(NewsContent):\n",
    "    dictTerm = list()\n",
    "    dictPos = list()\n",
    "    dictNoun = list()\n",
    "    dictNgram = list()\n",
    "    th = 1 \n",
    "    \n",
    "    for sentence in sent_tokenize(NewsContent):\n",
    "        for token in word_tokenize(sentence):\n",
    "            if len(token) > th:\n",
    "                dictTerm.append(token)\n",
    "                # 아래서부터는 list이기때문에 extend를 사용해야 함. \n",
    "                dictPos.extend([morpheme for morpheme in ma.morphs(token) if len(morpheme) > th]) # 형태소 분석결과를 extend\n",
    "                dictNoun.extend([noun for noun in ma.nouns(token) if len(noun) > th]) # 명사 단위로 잘라 extend\n",
    "                dictNgram.extend(ngramUmjeol(token)) # 바이그램을 리턴\n",
    "                \n",
    "    # 빠른 속도 및 중복 데이터 처리를 위해 set사용 (유일한 단어만 남김)\n",
    "    dictTerm = list(set(dictTerm))\n",
    "    dictPos = list(set(dictPos))\n",
    "    dictNoun = list(set(dictNoun))\n",
    "    dictNgram = list(set(dictNgram))\n",
    "        \n",
    "    return dictTerm, dictPos, dictNoun, dictNgram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenizedNewsContent = list()\n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    TokenizedNewsContent.append(Tokenizer(NewsContent[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276 214 181 484\n"
     ]
    }
   ],
   "source": [
    "# len(dictTerm), len(dictPos), len(dictNoun), len(dictNgram) of 0th txt file\n",
    "print(len(TokenizedNewsContent[0][0]), len(TokenizedNewsContent[0][1]), len(TokenizedNewsContent[0][2]), len(TokenizedNewsContent[0][3]))\n",
    "\n",
    "# dictTerm, dictPos, dictNoun, dictNgram 테스트용\n",
    "# for i in range(len(FileList)):\n",
    "#     print(str(i) + \"th txt : \" + str(len(list(set(TokenizedNewsContent[i][0] + TokenizedNewsContent[i][1] + TokenizedNewsContent[i][2] + TokenizedNewsContent[i][3])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['않아', '있다', '4년전', '가톨릭대', '확산되고', '국내외', '경북대', '다른', '건강', '했다', '190명의', '생물학', '부정하며', '이메일', '어리둥절', '세계보건기구', '경우', '우려가', '언론의', '2만5000개가', '현재는', '모스코위츠', '연구는', '오전', '근거로', '장시간', '필요하다는', '법적', '안정성에', '무선이어폰', '지속적으로', '같은', '않다', '서명하라', '전자기장의', '확실히', '세계', '내용에', '지방국립대', '2015년', '중앙대', '2년전', '통신용', '마이크로파가', '유엔환경계획', '발상도', '지난', '제출했다고', '5월', '비전리', '나오면서', '주도한', '부정확한', '블루투스의', '내용이다', '호소문은', '않은', '연구자도', '제출된', '하지만', '기준치를', '이메일로', '이라며', '영향을', '있지', '247명이', '있다며', '이들', '있는데', '했다고', '교수는', '보호와', '건강에', '결론내렸다', '호소문', '자기장에', '관련', '작성을', '시의', '연구들을', '과학자들의', '단국대', '단체측', '단체와', '따르면', '단체는', '서명을', '애플', '연구를', '동의하는', '15명도', '전했다', '준수하고', '좋지', '제품이나', '한림대', '올린', '연구들이', '존재하지만', '유해성을', '국제연합', '과학자들에게', '국제', '학술지에', '담고', '없다고', '소장은', '한국의', '난다', '42개국', '국내', '관한', '존재치도', '에어팟이', '언급하고', '올라간', '아닌', '생화학과', '불안감이', '가정사회건강연구소', '관련된', '않을', '이름도', '서강대', '것이며', '호소문을', '확인됐다', '당시', '주장하지', '2000개가', '내용이', '포함됐다', '방지가', '교수가', '18일', '보내온', '유발할', '이어폰이', '발생', '과학자', '과학자들은', '주제에', '서명했다', '있지만', '전자기장', '취재', '제출하기는', '보도했다', '과학자들이', '주변에', '인터뷰에서', '매체들은', '미치는지', '관계자인', '호소문에는', '1212', '자체', '언급하지', '인체에', '지나다니고', '담았다', '캘리포니아대', '이엠에프사이언티스트에', '머리가', '실제', '송기원', '오히려', '우리', '서명했던', '데일리메일과', '이어폰에', '저명한', '서명자인', '보도를', '에어팟과', '소속', '조엘', '말했다', '낮은', '서명한', '무선이어폰을', '한양대', '전자기장과', '동안', '존재하지', '키울', '않는다', '이어폰의', '참여', '아직까지', '한국인', '언론에', '비영리단체', '관여하지', '일부', '문제가', '기억이', '가져간다고', '위험을', '호소문에', '이엠에프사이언티스트', '부정했다', '있다는', '특정', '대략', '가까이', '동아사이언스가', '이엠에프사이언티스에서', '암을', '미국', '유해성에', '건강과', '서명했다고', '대한', '노출로부터', '게재된', '늘어났다', '해당', '여기에는', '247명으로', '못했다고', '연세대', '밝혔다', '실제로', '무선', '비이온화', '새로', '직접', '제출됐으며', '명단이', '몰랐다', '최근', '30년간', '새삼', '된다는', '이엠에프사이언티스', '확인한', '여부는', '이덕환', '수많은', '15명의', '여러', '논란이', '위험', '포함됐다고', '검증됐다', '당시도', '노출될', '것도', '전화나', '것으로', '보도가', '제출했다는', '받아', '황당하다', '알려진', '이번에', '내용을', '않았고', '과학자들', '확인하지', '화학과', '발표한', '이미', '넘는', '버클리', '고재원', '결과', '언론', '이름을', '분들', '제조사를', '전세계', '유해성', '제출됐던', '통해']\n"
     ]
    }
   ],
   "source": [
    "# 0 : dictTerm, 1 : dictPos, 2 : dictNoun, 3 : dictNgram\n",
    "print(TokenizedNewsContent[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TDF-ID 방식으로 검색 수행해보기\n",
    "WIP(2019-03-29 10:28 PM) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정\n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    localPosting = dict()\n",
    "    \n",
    "    # Local 작업\n",
    "    for term in docContent.lower().split():\n",
    "        # Local에 대해서, 없으면 추가\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "     \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for indexTerm, termFreq in localPosting.items():\n",
    "        if indexTerm not in queryPosting.keys(): \n",
    "            lexiconIdx = len(queryPosting)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, -1)\n",
    "            globalPosting.append(postingData)\n",
    "            queryPosting[indexTerm] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기\n",
    "            lexiconIdx = list(queryPosting.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = queryPosting[indexTerm]\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, beforeIdx)\n",
    "            globalPosting.append(postingData)\n",
    "            queryPosting[indexTerm] = postingIdx\n",
    "            \n",
    "#     print(localPosting)\n",
    "# print(globalDocument)\n",
    "\n",
    "#         if term not in globalLexicon.keys():\n",
    "#             lexiconIdx = len(globalLexicon) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts//IT과학-0000003610.txt', ' 오히려 단체측 무선이어폰 연구는 존재치도 않아 국내 참여 연구자도 어리둥절 이엠에프사이언티스에서 일부 언론 보도를 부정하며 직접 보내온 이메일 내용이다 이엠에프사이언티스트 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 무선 이어폰의 건강 유해성에 대한 보도를 부정했다 고재원 1212 18일 오전 전세계 과학자들이 애플 에어팟과 같은 무선 이어폰이 암 발생 위험을 키울 수 있다는 호소문을 국제연합 과 세계보건기구 에 제출했다는 일부 국내외 언론의 보도가 나오면서 불안감이 확산되고 있다 하지만 실제 이 단체와 호소문에 이름을 올린 과학자들에게 확인한 결과 호소문은 4년전 제출됐던 것이며 또 특정 제품이나 제조사를 언급하지 않은 것으로 확인됐다 18일 데일리메일과 등 국내외 일부 언론에 따르면 전 세계 과학자 247명이 무선 이어폰의 비이온화 전자기장 이 암을 유발할 위험 우려가 있다며 과 에 호소문을 제출했다고 전했다 이들 매체들은 호소문에 애플 에어팟이 에 관한 법적 기준치를 준수하고 있지만 장시간 노출될 경우 건강에 좋지 않을 수 있다 는 내용이 포함됐다고 보도했다 이들 매체들은 이 호소문에는 전 세계 42개국 과학자 247명이 서명을 했다고 전했다 여기에는 한국의 연세대 한양대 가톨릭대 단국대 중앙대 경북대 한림대 소속 과학자 15명의 이름도 포함됐다 하지만 취재 결과 호소문 작성을 주도한 비영리단체 이엠에프사이언티스트 는 애플 에어팟과 같은 무선 이어폰에 대한 유해성을 주장하지 않은 것으로 확인됐다 동아사이언스가 이엠에프사이언티스트에 직접 이메일로 확인한 결과 일부 언론 보도가 호소문에 대한 부정확한 내용을 담고 있다 며 무선 블루투스의 자기장에 머리가 장시간 노출될 시의 안정성에 대한 연구는 존재하지 않는다 고 밝혔다 이엠에프사이언티스트에 따르면 이 단체는 지난 2015년 5월 실제로 전세계 과학자 190명의 서명을 받아 과 유엔환경계획 에 국제 과학자 호소문 을 제출하기는 했다 당시 호소문에는 전세계 저명한 학술지에 게재된 2000개가 넘는 연구들을 근거로 비전리 전자기장 노출로부터 보호와 방지가 필요하다는 내용을 담았다 이 단체는 지속적으로 해당 내용에 동의하는 과학자들의 서명을 받아 현재는 42개국 247명으로 늘어났다 실제 한국인 과학자들 15명도 이 호소문에 서명한 것으로 확인됐다 이엠에프사이언티스 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 이메일 인터뷰에서 호소문에 서명한 과학자들은 저명한 학술지에 전자기장과 생물학 건강과 관련된 연구를 발표한 분들 이라며 하지만 호소문은 이번에 새로 제출된 내용이 아닌 2015년 5월 제출됐으며 특정 제품이나 제조사를 언급하고 있지 않다 고 말했다 이 호소문에 서명했다고 알려진 송기원 연세대 생화학과 교수는 명단이 올라간 것도 잘 몰랐다 며 최근 몇 년 동안 해당 주제에 관여하지 않았고 당시도 다른 교수가 서명하라 해 서명했다 고 말했다 또 다른 서명자인 한 지방국립대 교수는 대략 2년전 서명했던 기억이 난다 고 말했다 전자기장 자체 유해성 여부는 아직까지 논란이 있다 는 지난 30년간 2만5000개가 넘는 관련 연구들이 존재하지만 낮은 전자기장의 건강 유해성을 확인하지 못했다고 결론내렸다 이덕환 서강대 화학과 교수는 통신용 마이크로파가 인체에 영향을 미치는지 여부는 이미 여러 연구를 통해 문제가 없다고 확실히 검증됐다 며 이미 우리 주변에 수많은 통신용 마이크로파가 지나다니고 있는데 전화나 무선이어폰을 가까이 가져간다고 새삼 더 문제가 된다는 발상도 황당하다 고 밝혔다 고재원 1212 ')\n"
     ]
    }
   ],
   "source": [
    "collection = [] \n",
    "for i in range(len(NewsContent)):\n",
    "    collection.append((str(FileList[i]), NewsContent[i])) # (파일경로, 불용어 처리 완료된 기사) 튜플 구성 \n",
    "print(collection[0])\n",
    "\n",
    "# in-memory (Hash Key 값)\n",
    "# 전체 색인어 목록(Dictionary)\n",
    "# {단어1:포스팅위치, 단어2:포스팅위치, ...}\n",
    "queryPosting = dict()\n",
    "\n",
    "# 전체 문서 목록(Dictionary)\n",
    "# [0:문서1, 1:문서2, ...]\n",
    "globalDocument = list()\n",
    "\n",
    "# disk\n",
    "# 사전에 있는 색인어 중, 어느 문서에서, 몇 번 나타났는지\n",
    "# [(단어 idx, 문서 idx, 빈도, 다음주소), ...]\n",
    "# [0:Tuple(lexiconIdx, documentIdx, freq, 다음포스팅위치-fptr)]\n",
    "# 메모리 X, File OK\n",
    "globalPosting = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queryPosting, globalDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for indexTerm, postingIdx in queryPosting.items():\n",
    "    # indexTerm:단어: postingIdx:위치, ...\n",
    "    # print(indexTerm)\n",
    "    \n",
    "    while True: # Posting Next:-1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        # print('  DocName:{0} - TermFreq:{1} - Next:{2}'.format(globalDocument[postingData[1]], postingData[2], postingData[3]))\n",
    "        postingIdx = postingData[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 20, 2, 4199)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[queryPosting['이어폰']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(764, 15, 2, 798)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalPosting[queryPosting['이어폰']][3]]   # 다음 주소가 \"-1\" 일때 까지 반복해서 찾음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. TF-IDF 방식으로 검색해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log10\n",
    "\n",
    "# TF 구하기\n",
    "def binaryTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def rawTF(freq):\n",
    "    return freq\n",
    "\n",
    "def basicTF(freq, totalFreq):\n",
    "    return freq/totalFreq\n",
    "\n",
    "def logTF(freq):\n",
    "    if freq > 0:\n",
    "        return 1+log10(freq)\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def doubleNormalTF(K, freq, maxFreq): \n",
    "    return K + ((1-K) * (freq/maxFreq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF(Inverse Document Frequency 구하기\n",
    "def unaryIDF():\n",
    "    return 1\n",
    "\n",
    "def basicIDF(N, df):\n",
    "    return log10(N/df)\n",
    "\n",
    "def smoothigIDF(N, df):\n",
    "    return log10((N+1)/df)\n",
    "\n",
    "def probabilityIDF(N, df):\n",
    "    return log10((N-df+1)/df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "globalLexicon = dict()\n",
    "globalDocument = list()\n",
    "globalPosting = list()\n",
    "\n",
    "for (docName, docContent) in collection:\n",
    "    docIdx = len(globalDocument) # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정\n",
    "    globalDocument.append(docName)\n",
    "    localPosting = dict() # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    \n",
    "    for term in docContent.lower().split():\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "    \n",
    "    maxFreq = max(localPosting.values())\n",
    "    \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for indexTerm, termFreq in localPosting.items():\n",
    "        if indexTerm not in globalLexicon.keys():\n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, termFreq, maxFreq), -1]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기 \n",
    "            # tuple로 리턴받을경우 immutable(읽기전용)이라 list로 바꿔 주어야 함.\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = globalLexicon[indexTerm]\n",
    "            postingData = [lexiconIdx, docIdx, doubleNormalTF(0, termFreq, maxFreq), beforeIdx]\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(globalDocument)\n",
    "globalLexiconIDF = dict()\n",
    "\n",
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    df = 0\n",
    "    oldPostingIdx = postingIdx\n",
    "    \n",
    "    while True:  \n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "        \n",
    "        df += 1\n",
    "        postingData = globalPosting[postingIdx]\n",
    "        postingIdx = postingData[3]\n",
    "        \n",
    "    idf = basicIDF(N, df)\n",
    "    globalLexiconIDF[indexTerm] = idf\n",
    "    postingIdx = oldPostingIdx\n",
    "    \n",
    "    print('{0} / IDF-{1}'.format(indexTerm, idf))\n",
    "    \n",
    "    while True:\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "        \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        TF = postingData[2]\n",
    "        globalPosting[postingIdx][2] *= idf\n",
    "        print('    Documents:{0} / TF:{1} / TF-IDF:{2}'.format(globalDocument[postingData[1]], \n",
    "                                                               TF, globalPosting[postingIdx][2]))\n",
    "        \n",
    "        postingIdx = postingData[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
