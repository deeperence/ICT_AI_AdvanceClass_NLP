{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '미세', '먼지', '미세', '먼지']\n['오늘', '미세', '미세먼지', '먼지']\n['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요.']\n['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요', '.']\n원본 :  오늘 미세먼지는 어제 미세먼지보다 나빠요.\n어절 단위 분리 :  ['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요', '.']\n형태소 분석 결과 :  ['.', '미세먼지는', '아요', '보다', '는', '어제', '먼지', '나쁘', '미세먼지보다', '미세', '오늘', '나빠요']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['오늘',\n '미세먼지는',\n '어제',\n '미세먼지보다',\n '나빠요',\n '.',\n '오늘',\n '미세',\n '먼지',\n '는',\n '어제',\n '미세',\n '먼지',\n '보다',\n '나쁘',\n '아요',\n '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이 코드에서는 깨끗한 단어를 추출하기 위해 심화단계의 tokenize를 수행해 봅니다. (NLP를 위한 전처리)\n",
    "# 그동안 split()만 사용해 왔지만 다른 방법도 적용해 봅니다. \n",
    "\n",
    "from konlpy.tag import Kkma # 형태소 분석기 중에는 kkma가 성능이 제일 좋음.\n",
    "ma = Kkma() # 형태소 분석기 인스턴스\n",
    "sentence = \"오늘 미세먼지는 어제 미세먼지보다 나빠요.\"\n",
    "\n",
    "# 명사를 뽑으라고 시키면 => 오늘, 미세, 먼지, 어제, 미세, 먼지가 뽑혀야 함.\n",
    "# ma.pos(sentence) # 형태소가 부착된(태깅된) 형태로 반환\n",
    "print([token[0] for token in ma.pos(sentence) if token[1].startswith(\"NN\")])\n",
    "print(ma.nouns(sentence)) # 명사만 뽑기\n",
    "\n",
    "# BOW => index term, Lexicon(dictionary)로 부르기도 함. \n",
    "# 문장 단위 -> 어절 단위 -> 형태소 단위 -> 품사(명사) 단위로 보고, 추가로 Ngram 단위로도 문장을 볼 예정\n",
    "# 일반적인 전처리 순서 : 토크나이징 -> Normalization\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # 두 개의 토큰화 모듈 임포트 \n",
    "\n",
    "# 구두점에 대한 처리만 빼고 비슷한 결과가 나올 것임.\n",
    "print(sentence.split()) # 단순히 split\n",
    "print(word_tokenize(sentence)) # 구두점도 별도로 분류를 했으므로 어절이 누구인지 찾을 수 있다.\n",
    "\n",
    "print(\"원본 : \", sentence)\n",
    "\n",
    "# tokenized data\n",
    "lexicon = list()\n",
    "lexicon = word_tokenize(sentence)\n",
    "print(\"어절 단위 분리 : \", lexicon)\n",
    "\n",
    "# 품사에 대해 태깅 작업 수행\n",
    "# print(ma.pos(sentence)) # 둘의 차이는 그렇게 크지 않다.\n",
    "for token in word_tokenize(sentence): # 명사 분류시 문제가 될 수 있으므로 토큰화한 후 수행\n",
    "    lexicon.extend([token[0] for token in ma.pos(token)])\n",
    "print(\"형태소 분석 결과 : \", list(set(lexicon))) # 필요없는 중복값을 날리기 위해 set()에 담음 \n",
    "    \n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
