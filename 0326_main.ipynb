{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '미세', '먼지', '미세', '먼지']\n['오늘', '미세', '미세먼지', '먼지']\n['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요.']\n['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요', '.']\n원본 :  오늘 미세먼지는 어제 미세먼지보다 나빠요.\n어절 단위 분리 :  ['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요']\n형태소 분석 결과 :  ['미세먼지는', '아요', '보다', '어제', '먼지', '나쁘', '미세먼지보다', '미세', '오늘', '나빠요']\n명사 분석 결과 :  ['미세먼지는', '아요', '보다', '어제', '미세먼지', '먼지', '나쁘', '미세먼지보다', '미세', '오늘', '나빠요']\n어절(바이그램) 분석 :  ['미세', '나빠요', '미세먼지는', '먼지 보다', '는 어제', '보다', '미세 먼지', '먼지 는', '오늘', '오늘 미세', '어제', '미세먼지', '먼지', '보다 나쁘', '미세먼지보다', '아요', '아요 .', '나쁘 아요', '나쁘', '어제 미세']\n음절(바이그램) 분석 :  ['나빠', '지보', '미세', '나빠요', '세먼', '미세먼지는', '먼지 보다', '는 어제', '보다', '미세 먼지', '지는', '먼지 는', '오늘', '오늘 미세', '어제', '미세먼지', '먼지', '보다 나쁘', '미세먼지보다', '아요', '아요 .', '나쁘 아요', '빠요', '나쁘', '어제 미세']\n25\n"
     ]
    }
   ],
   "source": [
    "# 이 코드에서는 깨끗한 단어를 추출하기 위해 심화단계의 tokenize를 수행해 봅니다. (NLP를 위한 전처리)\n",
    "# 그동안 split()만 사용해 왔지만 다른 방법도 적용해 봅니다. 어느 feature가 좋은지 모르므로 최대한 많이 뽑아내는 것이 목적입니다. \n",
    "\n",
    "from konlpy.tag import Kkma # 형태소 분석기 중에는 kkma가 성능이 제일 좋음.\n",
    "ma = Kkma() # 형태소 분석기 인스턴스\n",
    "sentence = \"오늘 미세먼지는 어제 미세먼지보다 나빠요.\"\n",
    "\n",
    "# 명사를 뽑으라고 시키면 => 오늘, 미세, 먼지, 어제, 미세, 먼지가 뽑혀야 함.\n",
    "# ma.pos(sentence) # 형태소가 부착된(태깅된) 형태로 반환\n",
    "print([token[0] for token in ma.pos(sentence) if token[1].startswith(\"NN\")])\n",
    "print(ma.nouns(sentence)) # 명사만 뽑기\n",
    "\n",
    "# BOW => index term, Lexicon(dictionary)로 부르기도 함. \n",
    "# 문장 단위 -> 어절 단위 -> 형태소 단위 -> 품사(명사) 단위로 보고, 추가로 Ngram 단위로도 문장을 볼 예정\n",
    "# 일반적인 전처리 순서 : 토크나이징 -> Normalization(단어의 길이, 한국어의 경우 1음절 단위로 수행하기도 함)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # 두 개의 토큰화 모듈 임포트 \n",
    "\n",
    "# 구두점에 대한 처리만 빼고 비슷한 결과가 나올 것임.\n",
    "print(sentence.split()) # 단순히 split\n",
    "print(word_tokenize(sentence)) # 구두점도 별도로 분류를 했으므로 어절이 누구인지 찾을 수 있다.\n",
    "\n",
    "print(\"원본 : \", sentence)\n",
    "\n",
    "# tokenized data\n",
    "lexicon = list()\n",
    "th = 1 # 1음절보다 큰 문자를 고르기 위한 상수\n",
    "lexicon = [token for token in word_tokenize(sentence) if len(token) > th]\n",
    "print(\"어절 단위 분리 : \", lexicon)\n",
    "\n",
    "# 품사에 대해 태깅 작업 수행\n",
    "# print(ma.pos(sentence)) # 둘의 차이는 그렇게 크지 않다.\n",
    "for token in [token for token in word_tokenize(sentence) if len(token) > th]: # 명사 분류시 문제가 될 수 있으므로 토큰화한 후 수행\n",
    "    lexicon.extend([token[0] for token in ma.pos(token) if len(token[0]) > th]) # (단어, 품사) 튜플쌍으로 받기 위해 for문을 돌면서 형태소 분석. (ma.morphs() 함수를 사용하면 형태소만 반환받을 수 있음. )\n",
    "    # lexicon.extend([token[0] for token in ma.pos(token) if len(token[0]) > th] and token[1] in [\"NN\", \"NNG\"]) 등과 같이 and조건 옆에 원하는 품사를 넣어서 그 POS 태그에 해당하는 것들만 튜플쌍으로 받을 수도 있다. \n",
    "print(\"형태소 분석 결과 : \", list(set(lexicon))) # 필요없는 중복값을 날리기 위해 set()에 담음 \n",
    "\n",
    "for token in [token for token in word_tokenize(sentence) if len(token) > th]: # 명사 분류시 문제가 될 수 있으므로 토큰화한 후 수행\n",
    "    lexicon.extend(ma.nouns(token))\n",
    "print(\"명사 분석 결과 : \", list(set(lexicon))) # 필요없는 중복값을 날리기 위해 set()에 담음 \n",
    "# 더 세밀한 분석을 위해 N-gram을 적용할 필요가 있다.\n",
    "\n",
    "lexicon.extend(ngramEojeol(\" \".join([token[0] for token in ma.pos(sentence)])))\n",
    "print(\"어절(바이그램) 분석 : \", list(set(lexicon)))\n",
    "\n",
    "newLexicon = list()\n",
    "for term in lexicon:\n",
    "    newLexicon.extend(ngramUmjeol(term))\n",
    "lexicon.extend([term for term in newLexicon if len(term.strip()) > th])\n",
    "print(\"음절(바이그램) 분석 : \", list(set(lexicon)))\n",
    "print(len(list(set(lexicon))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramEojeol(sentence, n=2): # sentence를 받아 어절 단위로 분리해주는 함수\n",
    "    '''\n",
    "    입력:     단어1,   단어2,   단어3,  단어4 : 4\n",
    "    출력(2) : 단어12,  단어23,  단어34 :        3 - n + 1\n",
    "    출력(3) : 단어123, 단어234         :        2 - n + 1\n",
    "    '''\n",
    "    tokens = sentence.split()\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram.append(' '.join(tokens[i:i + n]))    \n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramUmjeol(term, n = 2): # 음절 단위로 구분하는 함수. sentence를 받아 2개(n=2)씩 쪼갠다.\n",
    "\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(term) - n + 1):\n",
    "        # ngram.append(tokens_ngram[i:i+n]) # 방법1\n",
    "        # ngram.append(tuple(tokens_ngram[i:i+n])) # 방법2 (튜플로 반환 시 키값을 쓸 수 있음)\n",
    "        ngram.append(''.join(term[i:i + n])) # 방법3\n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "# path = \"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\" (상대경로)\n",
    "# path = \"0314_DownloadedNewstxts\" (절대경로)\n",
    "\n",
    "# fileids() => 말뭉치 목록을 리턴. 이 함수처럼 getFileList라는 함수를 만들자.  \n",
    "def getFileList(base = \"./\", ext = \"txt\"): # 아무것도 안했다면 base는 현재 경로\n",
    "    fileList = list()\n",
    "    for file in listdir(base):\n",
    "        if file.split(\".\")[-1] == ext: # .을 기준으로 split한 것이 txt인지 검사\n",
    "            fileList.append(\"{0}/{1}\".format(base, file))\n",
    "            \n",
    "    return fileList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContent(file): # txt 컨텐츠를 편하게 읽어오기 위한 함수\n",
    "    with open(file, encoding=\"UTF-8\") as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "180"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(getFileList(\"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\")) # 정상작동하는지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = getContent(getFileList(\"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\")[0]) # 정상작동하는지 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~\n",
    "'\\n\\n\\n\\n\\n// flash \\nfunction _flash_removeCallback() {}\\n\\n jawon1212@donga.com]\\n\\n'와 같이 거슬리는 단어를 정규식을 응용해 걸러낼 필요가 있다. \n",
    "~~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~와 같은 불용어가 들어 있다. \n",
    "# 문자를 기입할때는 보통 []로 감싸서 사용. \n",
    "\n",
    "# 패턴의 리스트를 돌려주는 함수(key로 pattern을 호출해서 re.sub을 적용하기 위한 목적)\n",
    "def getPatternList():\n",
    "    patternList = dict()\n",
    "    pattern = re.compile(r\"[%s]{2,}\" % re.escape(punctuation)) # punctuation 안의 특수문자가 두번이상 반복되는 모든 문자에 대해 패턴 정의\n",
    "    patternList[\"Punctuation\"] = pattern\n",
    "    pattern2 = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{3,}@[A-Za-z0-9\\-\\_]{3,}(.[A-Za-z]{2,})+)\") #이메일주소제거패턴\n",
    "    patternList[\"Email\"] = pattern2\n",
    "    pattern3 = re.compile(r\"([A-Za-z0-9\\-\\_]{1,}(.[A-Za-z]{2,})+)\") # 신문사도메인패턴\n",
    "    patternList[\"Domain\"] = pattern3\n",
    "    pattern4 = re.compile(r\"\\s{2,}\") # 공백제거\n",
    "    patternList[\"Whitespace\"] = pattern4\n",
    "    pattern5 = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣0-9]+)\") # 한글이 아닌 영어 기호 제거\n",
    "    patternList[\"Korean\"] = pattern5\n",
    "    \n",
    "    return patternList\n",
    "    \n",
    "    \n",
    "# for _ in [\"Email\", \"Domain\", \"Nonword\", \"Punctuation\", \"Whitespace\"]:\n",
    "    \n",
    "    \n",
    "# # 첫번째 필터링 : 특수문자 제거\n",
    "# pattern = re.compile(r\"[%s]{2,}\" % re.escape(punctuation)) # punctuation 안의 특수문자가 두번이상 반복되는 모든 문자에 대해 패턴 정의 \n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "# print(\"1st pattern : \",  pattern) # 패턴이 잘 컴파일되었는지 확인 목적  \n",
    "# print(\"1st findall result : \", pattern.findall(content)) # 패턴대로 잘 찾아지는지 확인 목적\n",
    "# print(\"1st filtered result : \", pattern.sub(\" \", content)) # 정의된 패턴을 통해 2번이상 반복되는 특수 문자를 제거한 결과물(\" \"으로 치환한 결과) 출력\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "# \n",
    "# # 두번째 필터링 : 이메일 주소 제거\n",
    "# pattern2 = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{3,}@[A-Za-z0-9\\-\\_]{3,}(.[A-Za-z]{2,})+)\") # 이메일은 한글을 사용하지 못하므로 영어, 숫자, 하이픈, . 등만 존재할 것이다. 그 다음 @이 나옴.\n",
    "# print(\"2nd pattern : \",  pattern2)\n",
    "# print(\"2nd findall result : \", pattern2.search(content)) # 매치되는 딱 하나의 결과만 리턴\n",
    "# print(\"2nd filtered result : \", pattern2.sub(\" \", content))\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "# \n",
    "# # 세번째 필터링 : 언론사 도메인 주소 제거 (ex. www.khan.co.kr)\n",
    "# pattern3 = re.compile(r\"([A-Za-z0-9\\-\\_]{1,}(.[A-Za-z]{2,})+)\") # www.domain.com일 경우, www과 같은 서브도메인이 붙고 후속 내용들이 뒤따르는 형태. \n",
    "# print(\"3rd pattern : \",  pattern3)\n",
    "# print(\"3rd findall result : \", pattern3.findall(content)) \n",
    "# print(\"3rd filtered result : \", pattern3.sub(\" \", content))\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "# \n",
    "# # 네번째 필터링 : 화이트스페이스 제거\n",
    "# pattern4 = re.compile(r\"\\s{2,}\") # 공백이 두번이상 반복되는 케이스에 대한 패턴 정의\n",
    "# print(\"4th pattern : \",  pattern4)\n",
    "# print(\"4th findall result : \", pattern4.findall(content)) \n",
    "# print(\"4th filtered result : \", pattern4.sub(\" \", content))\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "# \n",
    "# # 다섯번째 필터링 : 한글이 아닌 영어 및 기호 제거\n",
    "# pattern5 = re.compile(r\"([^ㄱ_ㅎㅏ_ㅣ가_힣0-9]+)\")\n",
    "# print(\"5th pattern : \",  pattern5)\n",
    "# print(\"5th findall result : \", pattern5.findall(content)) \n",
    "# print(\"5th filtered result : \", pattern5.sub(\" \", content))\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# _flash_remoceCallback같은 거슬리는 영어 문구 제거(보통 EMFscientiest와 같이 특별한 의미를 갖고 있는 영단어 때문에 사용하면 위험함)\n",
    "# pattern = re.compile(r\"[A-Za-z\\-\\_]{4,}\") # 4글자 이상 반복되는 영어 문구 찾아내기\n",
    "# print(\"pattern : \",  pattern)\n",
    "# print(\"findall result : \", pattern.findall(content))\n",
    "# print(\"filtered result : \", pattern.sub(\" \", content))\n",
    "# print(\"------------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 오류를 우회하기 위한 함수 추가 오히려 단체측 무선이어폰 연구는 존재치도 않아 국내 참여 연구자도 어리둥절 이엠에프사이언티스에서 일부 언론 보도를 부정하며 직접 보내온 이메일 내용이다 이엠에프사이언티스트 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 무선 이어폰의 건강 유해성에 대한 보도를 부정했다 고재원 기자 1212 18일 오전 전세계 과학자들이 애플 에어팟과 같은 무선 이어폰이 암 발생 위험을 키울 수 있다는 호소문을 국제연합 과 세계보건기구 에 제출했다는 일부 국내외 언론의 보도가 나오면서 불안감이 확산되고 있다 하지만 실제 이 단체와 호소문에 이름을 올린 과학자들에게 확인한 결과 호소문은 4년전 제출됐던 것이며 또 특정 제품이나 제조사를 언급하지 않은 것으로 확인됐다 18일 데일리메일과 중앙일보 등 국내외 일부 언론에 따르면 전 세계 과학자 247명이 무선 이어폰의 비이온화 전자기장 이 암을 유발할 위험 우려가 있다며 과 에 호소문을 제출했다고 전했다 이들 매체들은 호소문에 애플 에어팟이 에 관한 법적 기준치를 준수하고 있지만 장시간 노출될 경우 건강에 좋지 않을 수 있다 는 내용이 포함됐다고 보도했다 이들 매체들은 이 호소문에는 전 세계 42개국 과학자 247명이 서명을 했다고 전했다 여기에는 한국의 연세대 한양대 가톨릭대 단국대 중앙대 경북대 한림대 소속 과학자 15명의 이름도 포함됐다 하지만 취재 결과 호소문 작성을 주도한 비영리단체 이엠에프사이언티스트 는 애플 에어팟과 같은 무선 이어폰에 대한 유해성을 주장하지 않은 것으로 확인됐다 동아사이언스가 이엠에프사이언티스트에 직접 이메일로 확인한 결과 일부 언론 보도가 호소문에 대한 부정확한 내용을 담고 있다 며 무선 블루투스의 자기장에 머리가 장시간 노출될 시의 안정성에 대한 연구는 존재하지 않는다 고 밝혔다 이엠에프사이언티스트에 따르면 이 단체는 지난 2015년 5월 실제로 전세계 과학자 190명의 서명을 받아 과 유엔환경계획 에 국제 과학자 호소문 을 제출하기는 했다 당시 호소문에는 전세계 저명한 학술지에 게재된 2000개가 넘는 연구들을 근거로 비전리 전자기장 노출로부터 보호와 방지가 필요하다는 내용을 담았다 이 단체는 지속적으로 해당 내용에 동의하는 과학자들의 서명을 받아 현재는 42개국 247명으로 늘어났다 실제 한국인 과학자들 15명도 이 호소문에 서명한 것으로 확인됐다 이엠에프사이언티스 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 이메일 인터뷰에서 호소문에 서명한 과학자들은 저명한 학술지에 전자기장과 생물학 건강과 관련된 연구를 발표한 분들 이라며 하지만 호소문은 이번에 새로 제출된 내용이 아닌 2015년 5월 제출됐으며 특정 제품이나 제조사를 언급하고 있지 않다 고 말했다 이 호소문에 서명했다고 알려진 송기원 연세대 생화학과 교수는 명단이 올라간 것도 잘 몰랐다 며 최근 몇 년 동안 해당 주제에 관여하지 않았고 당시도 다른 교수가 서명하라 해 서명했다 고 말했다 또 다른 서명자인 한 지방국립대 교수는 대략 2년전 서명했던 기억이 난다 고 말했다 전자기장 자체 유해성 여부는 아직까지 논란이 있다 는 지난 30년간 2만5000개가 넘는 관련 연구들이 존재하지만 낮은 전자기장의 건강 유해성을 확인하지 못했다고 결론내렸다 이덕환 서강대 화학과 교수는 통신용 마이크로파가 인체에 영향을 미치는지 여부는 이미 여러 연구를 통해 문제가 없다고 확실히 검증됐다 며 이미 우리 주변에 수많은 통신용 마이크로파가 지나다니고 있는데 전화나 무선이어폰을 가까이 가져간다고 새삼 더 문제가 된다는 발상도 황당하다 고 밝혔다 고재원 기자 1212 \n"
     ]
    }
   ],
   "source": [
    "patternList = getPatternList()\n",
    "\n",
    "content = getContent(getFileList(\"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\")[0])\n",
    "for _ in [\"Korean\", \"Whitespace\"]:\n",
    "    content = patternList[_].sub(\" \", content)\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "len(sent_tokenize(content)) # 구두점이 사라져서 토큰화 못함.\n",
    "\n",
    "content = getContent(getFileList(\"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\")[0])\n",
    "for _ in [\"Email\", \"Domain\", \"Korean\", \"Punctuation\", \"Whitespace\"]:\n",
    "    content = patternList[_].sub(\" \", content)\n",
    "    \n",
    "termList = list()\n",
    "posList = list()\n",
    "nounList = list()\n",
    "ngramList = list()\n",
    "\n",
    "for sentence in sent_tokenize(content):\n",
    "    for token in word_tokenize(sentence):\n",
    "        if len(token) > th:\n",
    "            termList.append(token)\n",
    "            # 아래서부터는 list이기때문에 extend를 사용해야 함. \n",
    "            posList.extend([morpheme for morpheme in ma.morphs(token) if len(morpheme) > th]) # 형태소 분석결과를 extend\n",
    "            nounList.extend([noun for noun in ma.nouns(token) if len(noun) > th]) # 명사 단위로 잘라 extend\n",
    "            ngramList.extend(ngramUmjeol(token)) # 바이그램을 리턴\n",
    "        \n",
    "# 빠른 속도를 위해 set사용\n",
    "termList = list(set(termList))\n",
    "posList = list(set(posList))\n",
    "nounList = list(set(nounList))\n",
    "ngramList = list(set(ngramList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(282, 219, 188, 492)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(termList), len(posList), len(nounList), len(ngramList) # (282, 219, 188, 492)\n",
    "\n",
    "# len(termList) : 순수하게 어절 단위로 자른 차원 수를 알 수 있고,(282차원)\n",
    "# th(길이)가 1보다 큰것들로 분류했으므로 은, 는, 이, 가와 같은 문자들이 사라졌을것. \n",
    "# len(posList), len(nounList)는 큰 차이가 없음. 즉 한국어는 명사 위주로 중요한 내용을 표현함을 알 수 있다. \n",
    "# ngram 모델의 단점 중 space complexity가 높다는 문제를 len(ngramList)을 통해 알 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "764"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문서가 크면 클수록 계속해서 선형적으로 늘어난다. 이런식으로 뽑아낸 대량의 lexicon을 두고 query에 대해 잘 retrieval하는 것이 목적. \n",
    "len(list(set(termList + posList + nounList + ngramList))) # 최종적으로 뽑은 term set (lexicon, index term list라고 부름.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
