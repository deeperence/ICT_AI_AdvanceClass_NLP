{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['오늘', '미세', '먼지', '미세', '먼지']\n['오늘', '미세', '미세먼지', '먼지']\n['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요.']\n['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요', '.']\n원본 :  오늘 미세먼지는 어제 미세먼지보다 나빠요.\n어절 단위 분리 :  ['오늘', '미세먼지는', '어제', '미세먼지보다', '나빠요']\n형태소 분석 결과 :  ['미세먼지는', '아요', '보다', '어제', '먼지', '나쁘', '미세먼지보다', '미세', '오늘', '나빠요']\n명사 분석 결과 :  ['미세먼지는', '아요', '보다', '어제', '미세먼지', '먼지', '나쁘', '미세먼지보다', '미세', '오늘', '나빠요']\n어절(바이그램) 분석 :  ['미세', '나빠요', '미세먼지는', '먼지 보다', '는 어제', '보다', '미세 먼지', '먼지 는', '오늘', '오늘 미세', '어제', '미세먼지', '먼지', '보다 나쁘', '미세먼지보다', '아요', '아요 .', '나쁘 아요', '나쁘', '어제 미세']\n음절(바이그램) 분석 :  ['나빠', '지보', '미세', '나빠요', '세먼', '미세먼지는', '먼지 보다', '는 어제', '보다', '미세 먼지', '지는', '먼지 는', '오늘', '오늘 미세', '어제', '미세먼지', '먼지', '보다 나쁘', '미세먼지보다', '아요', '아요 .', '나쁘 아요', '빠요', '나쁘', '어제 미세']\n25\n"
     ]
    }
   ],
   "source": [
    "# 이 코드에서는 깨끗한 단어를 추출하기 위해 심화단계의 tokenize를 수행해 봅니다. (NLP를 위한 전처리)\n",
    "# 그동안 split()만 사용해 왔지만 다른 방법도 적용해 봅니다. \n",
    "\n",
    "from konlpy.tag import Kkma # 형태소 분석기 중에는 kkma가 성능이 제일 좋음.\n",
    "ma = Kkma() # 형태소 분석기 인스턴스\n",
    "sentence = \"오늘 미세먼지는 어제 미세먼지보다 나빠요.\"\n",
    "\n",
    "# 명사를 뽑으라고 시키면 => 오늘, 미세, 먼지, 어제, 미세, 먼지가 뽑혀야 함.\n",
    "# ma.pos(sentence) # 형태소가 부착된(태깅된) 형태로 반환\n",
    "print([token[0] for token in ma.pos(sentence) if token[1].startswith(\"NN\")])\n",
    "print(ma.nouns(sentence)) # 명사만 뽑기\n",
    "\n",
    "# BOW => index term, Lexicon(dictionary)로 부르기도 함. \n",
    "# 문장 단위 -> 어절 단위 -> 형태소 단위 -> 품사(명사) 단위로 보고, 추가로 Ngram 단위로도 문장을 볼 예정\n",
    "# 일반적인 전처리 순서 : 토크나이징 -> Normalization(단어의 길이, 한국어의 경우 1음절 단위로 수행하기도 함)\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize # 두 개의 토큰화 모듈 임포트 \n",
    "\n",
    "# 구두점에 대한 처리만 빼고 비슷한 결과가 나올 것임.\n",
    "print(sentence.split()) # 단순히 split\n",
    "print(word_tokenize(sentence)) # 구두점도 별도로 분류를 했으므로 어절이 누구인지 찾을 수 있다.\n",
    "\n",
    "print(\"원본 : \", sentence)\n",
    "\n",
    "# tokenized data\n",
    "lexicon = list()\n",
    "th = 1 # 1음절보다 큰 문자를 고르기 위한 상수\n",
    "lexicon = [token for token in word_tokenize(sentence) if len(token) > th]\n",
    "print(\"어절 단위 분리 : \", lexicon)\n",
    "\n",
    "# 품사에 대해 태깅 작업 수행\n",
    "# print(ma.pos(sentence)) # 둘의 차이는 그렇게 크지 않다.\n",
    "for token in [token for token in word_tokenize(sentence) if len(token) > th]: # 명사 분류시 문제가 될 수 있으므로 토큰화한 후 수행\n",
    "    lexicon.extend([token[0] for token in ma.pos(token) if len(token[0]) > th])\n",
    "print(\"형태소 분석 결과 : \", list(set(lexicon))) # 필요없는 중복값을 날리기 위해 set()에 담음 \n",
    "\n",
    "for token in [token for token in word_tokenize(sentence) if len(token) > th]: # 명사 분류시 문제가 될 수 있으므로 토큰화한 후 수행\n",
    "    lexicon.extend(ma.nouns(token))\n",
    "print(\"명사 분석 결과 : \", list(set(lexicon))) # 필요없는 중복값을 날리기 위해 set()에 담음 \n",
    "# 더 세밀한 분석을 위해 N-gram을 적용할 필요가 있다.\n",
    "\n",
    "lexicon.extend(ngramEojeol(\" \".join([token[0] for token in ma.pos(sentence)])))\n",
    "print(\"어절(바이그램) 분석 : \", list(set(lexicon)))\n",
    "\n",
    "newLexicon = list()\n",
    "for term in lexicon:\n",
    "    newLexicon.extend(ngramUmjeol(term))\n",
    "lexicon.extend([term for term in newLexicon if len(term.strip())>th])\n",
    "print(\"음절(바이그램) 분석 : \", list(set(lexicon)))\n",
    "print(len(list(set(lexicon))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramEojeol(sentence, n=2):\n",
    "    '''\n",
    "    입력:     단어1,   단어2,   단어3,  단어4 : 4\n",
    "    출력(2) : 단어12,  단어23,  단어34 :        3 - n + 1\n",
    "    출력(3) : 단어123, 단어234         :        2 - n + 1\n",
    "    '''\n",
    "    tokens = sentence.split()\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram.append(' '.join(tokens[i:i + n]))    \n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramUmjeol(term, n=2): # 음절 단위로 구분하는 함수. sentence를 받아 2개(n=2)씩 쪼갠다.\n",
    "\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(term) - n + 1):\n",
    "        # ngram.append(tokens_ngram[i:i+n]) # 방법1\n",
    "        # ngram.append(tuple(tokens_ngram[i:i+n])) # 방법2 (튜플로 반환 시 키값을 쓸 수 있음)\n",
    "        ngram.append(''.join(term[i:i + n])) # 방법3\n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
