{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "* 색인 & 가중치 => 학습\n",
    "1. 수집한 문서 목록 가져오기\n",
    "2. 각 문서에서 색인어 목록 추출 전처리 -> 어절 -> 형태소+명사+바이그램\n",
    "3. 가중치 계산(TF-IDF) -> TDM -> TWM(with DocumentVectorLength)\n",
    "\n",
    "* 질의 -> 색인과정의 2 to 3\n",
    "4. 질의에서 색인어 추출\n",
    "5. 가중치 계산\n",
    "6. 유사도 계산(코싸인)\n",
    "7. 유사도 순 정렬(거리-오름차순, 각도-내림차순)\n",
    "8. 검색 결과 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFileList(base = \"./\", ext = \"txt\"): # 아무것도 안했다면 base는 현재 경로\n",
    "    fileList = list()\n",
    "    for file in listdir(base):\n",
    "        if file.split(\".\")[-1] == ext: # .을 기준으로 split한 것이 txt인지 검사\n",
    "            fileList.append(\"{0}/{1}\".format(base, file))\n",
    "            \n",
    "    return fileList\n",
    "\n",
    "# txt 컨텐츠를 편하게 읽어오기 위한 함수\n",
    "def getContent(file): \n",
    "    with open(file, encoding=\"UTF-8\") as f:\n",
    "        content = f.read()\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramEojeol(sentence, n=2): # sentence를 받아 어절 단위로 분리해주는 함수\n",
    "    '''\n",
    "    입력:     단어1,   단어2,   단어3,  단어4 : 4\n",
    "    출력(2) : 단어12,  단어23,  단어34 :        3 - n + 1\n",
    "    출력(3) : 단어123, 단어234         :        2 - n + 1\n",
    "    '''\n",
    "    tokens = sentence.split()\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(tokens) - n + 1):\n",
    "        ngram.append(' '.join(tokens[i:i + n]))    \n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramUmjeol(term, n = 2): # 음절 단위로 구분하는 함수. sentence를 받아 2개(n=2)씩 쪼갠다.\n",
    "\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(term) - n + 1):\n",
    "        # ngram.append(tokens_ngram[i:i+n]) # 방법1\n",
    "        # ngram.append(tuple(tokens_ngram[i:i+n])) # 방법2 (튜플로 반환 시 키값을 쓸 수 있음)\n",
    "        ngram.append(''.join(term[i:i + n])) # 방법3\n",
    "        \n",
    "    return ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "\n",
    "\n",
    "def getPatternList():\n",
    "    patternList = dict()\n",
    "    pattern = re.compile(r\"[%s]{1,}\" % re.escape(punctuation)) # punctuation 안의 특수문자가 두번이상 반복되는 모든 문자에 대해 패턴 정의\n",
    "    patternList[\"Punctuation\"] = pattern\n",
    "    pattern2 = re.compile(r\"([A-Za-z0-9\\-\\_\\.]{3,}@[A-Za-z0-9\\-\\_]{3,}(.[A-Za-z]{2,})+)\") #이메일주소제거패턴\n",
    "    patternList[\"Email\"] = pattern2\n",
    "    pattern3 = re.compile(r\"([A-Za-z0-9\\-\\_]{1,}(.[A-Za-z]{2,})+)\") # 신문사도메인패턴\n",
    "    patternList[\"Domain\"] = pattern3\n",
    "    pattern4 = re.compile(r\"\\s{2,}\") # 공백제거\n",
    "    patternList[\"Whitespace\"] = pattern4\n",
    "    pattern5 = re.compile(r\"([^ㄱ-ㅎㅏ-ㅣ가-힣0-9]+)\") # 한글이 아닌 영어 기호 제거\n",
    "    patternList[\"Korean\"] = pattern5\n",
    "    pattern6 = re.compile(r'[A-Za-z-_]{8,}') # 영어 (대소문자) +\\-\\_ 제거 (8글자 이상)\n",
    "    patternList[\"ElimLongEng\"] = pattern6\n",
    "    pattern7 = re.compile(r\"를 2019 및 금지|네이버 채널에서|저작권자 주 블로터앤미디어 저작권자를 명기하고 내용을 변경하지 않으며 비상업적으로 이용하는 조건아래 가능합니다|\"\n",
    "                          r\"디지털 마케팅의 미래 아이포럼|무단전재|오류를 우회하기 위한 함수 추가|재배포|기자|아이뉴스24|구독해주세요|\"\n",
    "                          r\"고수들의 재테크 비법|이건희칼럼|성공을 꿈꾸는 사람들의 경제 뉴스|머니 및 금지|및 금지|\"\n",
    "                          r\"네이버 모바일에서|네이버에서 헤럴드경제|채널 구독하기|헤럴드 리얼라이프 헤럴드경제 사이트 바로가기 헤럴드경제 무단 전재 및 금지|\"\n",
    "                          r\"전자신문|바로가기|전자신문인터넷|인터넷 및 금지|중앙일보|친구추가|구독 1위|\"\n",
    "                          r\"한국경제|재배포|기사제보|보도자료|한경닷컴|네이버에서|자세히보기|자세히 보기|모바일한경|구독신청|\"\n",
    "                          r\"머니투데이|돈이 보이는 리얼타임 뉴스|연예뉴스|파이낸셜뉴스|무단 전재 금지\") # 높은 가중치를 갖는 단어를 하드코딩 데이터로 제거\n",
    "    patternList[\"ElimRecWord\"] = pattern7\n",
    "    \n",
    "    return patternList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "FileList = getFileList(\"C:/Users/brsta/ICT_AI_AdvanceClass_NLP/0314_DownloadedNewstxts/\")\n",
    "NewsContent = list() \n",
    "\n",
    "for i in range(len(FileList)):\n",
    "    NewsContent.append(getContent(FileList[i])) # len(NewsContent) = 180\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 오히려 단체측 무선이어폰 연구는 존재치도 않아 국내 참여 연구자도 어리둥절 이엠에프사이언티스에서 일부 언론 보도를 부정하며 직접 보내온 이메일 내용이다 이엠에프사이언티스트 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 무선 이어폰의 건강 유해성에 대한 보도를 부정했다 고재원 1212 18일 오전 전세계 과학자들이 애플 에어팟과 같은 무선 이어폰이 암 발생 위험을 키울 수 있다는 호소문을 국제연합 과 세계보건기구 에 제출했다는 일부 국내외 언론의 보도가 나오면서 불안감이 확산되고 있다 하지만 실제 이 단체와 호소문에 이름을 올린 과학자들에게 확인한 결과 호소문은 4년전 제출됐던 것이며 또 특정 제품이나 제조사를 언급하지 않은 것으로 확인됐다 18일 데일리메일과 등 국내외 일부 언론에 따르면 전 세계 과학자 247명이 무선 이어폰의 비이온화 전자기장 이 암을 유발할 위험 우려가 있다며 과 에 호소문을 제출했다고 전했다 이들 매체들은 호소문에 애플 에어팟이 에 관한 법적 기준치를 준수하고 있지만 장시간 노출될 경우 건강에 좋지 않을 수 있다 는 내용이 포함됐다고 보도했다 이들 매체들은 이 호소문에는 전 세계 42개국 과학자 247명이 서명을 했다고 전했다 여기에는 한국의 연세대 한양대 가톨릭대 단국대 중앙대 경북대 한림대 소속 과학자 15명의 이름도 포함됐다 하지만 취재 결과 호소문 작성을 주도한 비영리단체 이엠에프사이언티스트 는 애플 에어팟과 같은 무선 이어폰에 대한 유해성을 주장하지 않은 것으로 확인됐다 동아사이언스가 이엠에프사이언티스트에 직접 이메일로 확인한 결과 일부 언론 보도가 호소문에 대한 부정확한 내용을 담고 있다 며 무선 블루투스의 자기장에 머리가 장시간 노출될 시의 안정성에 대한 연구는 존재하지 않는다 고 밝혔다 이엠에프사이언티스트에 따르면 이 단체는 지난 2015년 5월 실제로 전세계 과학자 190명의 서명을 받아 과 유엔환경계획 에 국제 과학자 호소문 을 제출하기는 했다 당시 호소문에는 전세계 저명한 학술지에 게재된 2000개가 넘는 연구들을 근거로 비전리 전자기장 노출로부터 보호와 방지가 필요하다는 내용을 담았다 이 단체는 지속적으로 해당 내용에 동의하는 과학자들의 서명을 받아 현재는 42개국 247명으로 늘어났다 실제 한국인 과학자들 15명도 이 호소문에 서명한 것으로 확인됐다 이엠에프사이언티스 관계자인 조엘 모스코위츠 미국 버클리 캘리포니아대 가정사회건강연구소 소장은 이메일 인터뷰에서 호소문에 서명한 과학자들은 저명한 학술지에 전자기장과 생물학 건강과 관련된 연구를 발표한 분들 이라며 하지만 호소문은 이번에 새로 제출된 내용이 아닌 2015년 5월 제출됐으며 특정 제품이나 제조사를 언급하고 있지 않다 고 말했다 이 호소문에 서명했다고 알려진 송기원 연세대 생화학과 교수는 명단이 올라간 것도 잘 몰랐다 며 최근 몇 년 동안 해당 주제에 관여하지 않았고 당시도 다른 교수가 서명하라 해 서명했다 고 말했다 또 다른 서명자인 한 지방국립대 교수는 대략 2년전 서명했던 기억이 난다 고 말했다 전자기장 자체 유해성 여부는 아직까지 논란이 있다 는 지난 30년간 2만5000개가 넘는 관련 연구들이 존재하지만 낮은 전자기장의 건강 유해성을 확인하지 못했다고 결론내렸다 이덕환 서강대 화학과 교수는 통신용 마이크로파가 인체에 영향을 미치는지 여부는 이미 여러 연구를 통해 문제가 없다고 확실히 검증됐다 며 이미 우리 주변에 수많은 통신용 마이크로파가 지나다니고 있는데 전화나 무선이어폰을 가까이 가져간다고 새삼 더 문제가 된다는 발상도 황당하다 고 밝혔다 고재원 1212 \n"
     ]
    }
   ],
   "source": [
    "patternList = getPatternList()\n",
    "for _ in [\"Korean\", \"Whitespace\", \"Punctuation\", \"ElimLongEng\", \"Email\", \"Domain\", \"ElimRecWord\"]: \n",
    "    for i in range(len(FileList)):\n",
    "        NewsContent[i] = patternList[_].sub(\" \", NewsContent[i]) \n",
    "print(NewsContent[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from konlpy.tag import Kkma\n",
    "ma = Kkma()\n",
    "# len(sent_tokenize(NewsContent[0])) # 구두점이 사라져서 토큰화 못함.\n",
    "\n",
    "dictTerm = list()\n",
    "dictPos = list()\n",
    "dictNoun = list()\n",
    "dictNgram = list()\n",
    "th=1\n",
    "\n",
    "for sentence in sent_tokenize(NewsContent[0]):\n",
    "    for token in word_tokenize(sentence):\n",
    "        if len(token) > th:\n",
    "            dictTerm.append(token)\n",
    "            # 아래서부터는 list이기때문에 extend를 사용해야 함. \n",
    "            dictPos.extend([morpheme for morpheme in ma.morphs(token) if len(morpheme) > th]) # 형태소 분석결과를 extend\n",
    "            dictNoun.extend([noun for noun in ma.nouns(token) if len(noun) > th]) # 명사 단위로 잘라 extend\n",
    "            dictNgram.extend(ngramUmjeol(token)) # 바이그램을 리턴\n",
    "            \n",
    "# 빠른 속도 및 중복 데이터 처리를 위해 set사용\n",
    "dictTerm = list(set(dictTerm))\n",
    "dictPos = list(set(dictPos))\n",
    "dictNoun = list(set(dictNoun))\n",
    "dictNgram = list(set(dictNgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictTerm), len(dictPos), len(dictNoun), len(dictNgram)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['이번', '장시간', '장과', '재원', '190', '교수', '영향', '기구', '대하', '제조사', '못하', '특정', '세계', '과학자', '나오', '비영리', '이미', '유엔', '신용', '기억', '실제로', '법적', '경북', '매체', '30', '하다', '호소문', '성에', '사이', '결과', '방지', '는데', '확산', '애플', '이어폰', '지방', '라며', '가져가', '에게', '부터', '여부', '연구소', '당시', '이나', '국내', '학술지', '늘어나', '언론', '준수', '사회', '계획', '발표', '어리둥절', '마이크로', '여기', '한국', '취재', '데일리', '5000', '검증', '주장', '주도', '다른', '한림', '자인', '하지만', '문제', '발생', '대략', '유해', '동의', '안정성', '오전', '유해성', '통하', '근거', '기준치', '연구자', '이메일', '지만', '관계자', '일부', '불안감', '전하', '주제', '에어팟', '위험', '보도', '올리', '지속적', '스트', '소속', '환경', '명단', '강대', '말하', '에서', '필요', '전세계', 'ㄴ다고', '개국', '가까이', '전자기', '주변', '참여', '미국', '이름', '따르', '내리', '현재', '생화', '최근', '티스', '사이언스', '버클리', '가톨릭대', '우리', '부정확', '자체', '인체', '동안', '중앙대', '국내외', '새삼', '국립대', '가정', '단국', '노출', '모스코', '건강', '메일', '보내오', '수많', '단체', '캘리포니아', '블루투스', '전화', '연합', '제출', '15', '자기장', 'ㄴ다', '해당', '시의', '밝히', '새로', '연세대', '으며', '지나다니', '다고', '아니', '다며', '관여', '다는', '비이온', '화학', '18', '국제', '키우', '부정', '제품', '여러', '오히려', '연구', '으로', '송기원', '한국인', '관련', '학과', '게재', '결론', '소장', '무선', '언급', '황당', '동아', '지나', '저명', '는지', '포함', '머리', '미치', '유발', 'ㄴ다는', '에프', '2015', '우려', '작성', '인터뷰', '직접', '생물학', '올라가', '면서', '양대', '실제', '보호', '비전', '존재', '1212', '경우', '논란', '발상', '아직', '2000', '장의', '서명', '42', '247', '확인', '내용', '확실히', '알리', '보건', '모르']\n"
     ]
    }
   ],
   "source": [
    "print(dictPos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "754"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len(list(set(dictTerm + dictPos + dictNoun + dictNgram)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['15명의', '장시간', '같은', '캘리포니아대', '서강대', '한림대', '제출됐으며', '불안감이', '확인한', '호소문은', '영향을', '언급하고', '나오면서', '있지', '확산되고', '특정', '세계', '과학자', '42개국', '이미', '무선이어폰', '존재치도', '언급하지', '단체는', '실제로', '법적', '과학자들의', '이덕환', '결과', '호소문', '이들', '포함됐다고', '있는데', '포함됐다', '전했다', '통신용', '내용이', '세계보건기구', '애플', '자기장에', '난다', '주변에', '2015년', '언론의', '제품이나', '건강에', '황당하다', '당시', '내용이다', '국내', '마이크로파가', '관계자인', '에어팟과', '따르면', '유발할', '우려가', '관한', '경북대', '게재된', '언론', '서명했다고', '동아사이언스가', '교수는', '이름도', '30년간', '올린', '호소문에는', '블루투스의', '아직까지', '어리둥절', '암을', '한양대', '취재', '인체에', '확실히', '소장은', '현재는', '다른', '좋지', '근거로', '않아', '주장하지', '필요하다는', '하지만', '제출됐던', '발생', '대략', '부정하며', '기준치를', '있지만', '연구를', '오전', '이메일로', '247명이', '서명했던', '유해성', '이엠에프사이언티스', '지방국립대', '된다는', '대한', '몰랐다', '단체측', '이메일', '지난', '않았고', '연구들이', '통해', '연구자도', '데일리메일과', '조엘', '부정했다', '동의하는', '일부', '190명의', '18일', '서명자인', '위험', '15명도', '않는다', '확인하지', '밝혔다', '키울', '전자기장', '아닌', '건강과', '소속', '247명으로', '담았다', '못했다고', '이엠에프사이언티스트', '명단이', '전세계', '부정확한', '늘어났다', '발표한', '검증됐다', '가까이', '주도한', '않다', '참여', '미국', '유해성을', '이번에', '발상도', '이어폰이', '최근', '우리', '버클리', '가톨릭대', '내용을', '받아', '했다', '자체', '이엠에프사이언티스에서', '했다고', '안정성에', '서명을', '중앙대', '국내외', '유엔환경계획', '동안', '미치는지', '새삼', '않을', '인터뷰에서', '기억이', '건강', '제출하기는', '가정사회건강연구소', '이어폰에', '연구는', '이엠에프사이언티스트에', '있다는', '모스코위츠', '과학자들', '언론에', '시의', '해당', '새로', '연세대', '4년전', '올라간', '것도', '서명했다', '확인됐다', '작성을', '호소문을', '에어팟이', '이라며', '국제', '여부는', '가져간다고', '여러', '오히려', '논란이', '수많은', '않은', '무선이어폰을', '한국의', '담고', '5월', '알려진', '관련된', '한국인', '송기원', '관련', '비영리단체', '제출했다고', '무선', '위험을', '보도가', '보호와', '지나다니고', '넘는', '보내온', '매체들은', '고재원', '주제에', '보도를', '생화학과', '2만5000개가', '화학과', '문제가', '유해성에', '것으로', '전화나', '당시도', '비전리', '보도했다', '직접', '과학자들에게', '과학자들은', '생물학', '결론내렸다', '과학자들이', '존재하지만', '것이며', '여기에는', '전자기장과', '노출로부터', '제조사를', '서명한', '제출된', '지속적으로', '이어폰의', '실제', '있다며', '존재하지', '있다', '관여하지', '교수가', '저명한', '분들', '내용에', '2년전', '1212', '경우', '머리가', '서명하라', '호소문에', '이름을', '낮은', '단체와', '준수하고', '노출될', '2000개가', '비이온화', '학술지에', '연구들을', '말했다', '없다고', '방지가', '제출했다는', '전자기장의', '단국대', '국제연합']\n"
     ]
    }
   ],
   "source": [
    "print(dictTerm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "collection = []\n",
    "# print(type(FileList[0]))\n",
    "\n",
    "for i in range(len(NewsContent)):\n",
    "    collection.append((str(FileList[i]), NewsContent[i]))\n",
    "print(type(collection[10][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-memory (Hash Key 값)\n",
    "# 전체 색인어 목록(Dictionary)\n",
    "# {단어1:포스팅위치, 단어2:포스팅위치, ...}\n",
    "globalLexicon = dict()\n",
    "\n",
    "# 전체 문서 목록(Dictionary)\n",
    "# [0:문서1, 1:문서2, ...]\n",
    "globalDocument = list()\n",
    "\n",
    "# disk\n",
    "# 사전에 있는 색인어 중, 어느 문서에서, 몇 번 나타났는지\n",
    "# [(단어 idx, 문서 idx, 빈도, 다음주소), ...]\n",
    "# [0:Tuple(lexiconIdx, documentIdx, freq, 다음포스팅위치-fptr)]\n",
    "# 메모리 X, File OK\n",
    "globalPosting = list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (docName, docContent) in collection:\n",
    "    # Pointer 대체용, Key, Document 이름은 절대로 겹치지 않는다는 가정. \n",
    "    docIdx = len(globalDocument)\n",
    "    globalDocument.append(docName)\n",
    "    \n",
    "    # {단어idx:빈도, 단어idx:빈도, ...}\n",
    "    localPosting = dict()\n",
    "    \n",
    "    # Local 작업\n",
    "    for term in docContent.lower().split():\n",
    "        # Local에 대해서 수행한 후 없으면 새 posting으로 추가\n",
    "        if term not in localPosting.keys():\n",
    "            localPosting[term] = 1 # dict\n",
    "        # 있으면, 빈도 증가\n",
    "        else:\n",
    "            localPosting[term] += 1\n",
    "     \n",
    "    # Global Marge\n",
    "    # fp -> struct(단어, 빈도) (localPosting)\n",
    "    for indexTerm, termFreq in localPosting.items(): # indexTerm : str,termFreq : int \n",
    "        if indexTerm not in globalLexicon.keys(): \n",
    "            lexiconIdx = len(globalLexicon)\n",
    "            postingIdx = len(globalPosting) # fseek\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, -1)\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx # globalPosting 위치(ptr:idx)\n",
    "        else: # 기존 단어의 idx 가져오기\n",
    "            lexiconIdx = list(globalLexicon.keys()).index(indexTerm)\n",
    "            postingIdx = len(globalPosting)\n",
    "            beforeIdx = globalLexicon[indexTerm]\n",
    "            postingData = (lexiconIdx, docIdx, termFreq, beforeIdx)\n",
    "            globalPosting.append(postingData)\n",
    "            globalLexicon[indexTerm] = postingIdx\n",
    "            \n",
    "#     print(localPosting)\n",
    "# print(globalDocument)\n",
    "\n",
    "#         if term not in globalLexicon.keys():\n",
    "#             lexiconIdx = len(globalLexicon) 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalLexicon, globalDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# globalPosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인용\n",
    "for indexTerm, postingIdx in globalLexicon.items():\n",
    "    # indexTerm:단어: postingIdx:위치, ...\n",
    "    # print(indexTerm)\n",
    "    \n",
    "    while True: # Posting Next:-1\n",
    "        if postingIdx == -1:\n",
    "            break\n",
    "            \n",
    "        postingData = globalPosting[postingIdx]\n",
    "        # print('  DocName:{0} - TermFreq:{1} - Next:{2}'.format(globalDocument[postingData[1]], postingData[2], postingData[3]))\n",
    "        postingIdx = postingData[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(925, 25, 1, 4383)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globalPosting[globalLexicon['출시가']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from konlpy.tag import Kkma\n",
    "ma = Kkma()\n",
    "\n",
    "dictTerm = list()\n",
    "dictPos = list()\n",
    "dictNoun = list()\n",
    "dictNgram = list()\n",
    "th = 1\n",
    "\n",
    "for sentence in sent_tokenize(NewsContent[0]):\n",
    "    for token in word_tokenize(sentence):\n",
    "        if len(token) > th:\n",
    "            dictTerm.append(token)\n",
    "            # 아래서부터는 list이기때문에 extend를 사용해야 함. \n",
    "            dictPos.extend([morpheme for morpheme in ma.morphs(token) if len(morpheme) > th]) # 형태소 분석결과를 extend\n",
    "            dictNoun.extend([noun for noun in ma.nouns(token) if len(noun) > th]) # 명사 단위로 잘라 extend\n",
    "            dictNgram.extend(ngramUmjeol(token)) # 바이그램을 리턴\n",
    "            \n",
    "# 빠른 속도 및 중복 데이터 처리를 위해 set사용\n",
    "dictTerm = list(set(dictTerm))\n",
    "dictPos = list(set(dictPos))\n",
    "dictNoun = list(set(dictNoun))\n",
    "dictNgram = list(set(dictNgram))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
